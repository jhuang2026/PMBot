{
  "file_path": "documents/general/Red Hat AI_ Our vision, strategy and roadmap- RTX Corporation.pdf",
  "product": "general",
  "product_name": "Red Hat AI General",
  "filename": "Red Hat AI_ Our vision, strategy and roadmap- RTX Corporation.pdf",
  "content": "## Red Hat AI: Our vision, strategy and roadmap\n\n<!-- image -->\n\n<!-- image -->\n\n## AI is a strategic enabler across industries\n\nAI use cases that drive productivity and efficiency\n\nEvery vertical industry\n\n<!-- image -->\n\n<!-- image -->\n\nEvery business function\n\n<!-- image -->\n\n## Operationalizing AI is still a challenging process\n\nWhat is the average AI/ML timeline from idea to operationalizing the model?\n\nHalf of respondents (50%) say their average AI/ML timeline from idea to operationalizing the model is 7-12 months.\n\n<!-- image -->\n\n<!-- image -->\n\n## AI has undergone significant evolution\n\nThe evolution of AI: from Business Intelligence to Generative AI\n\n- ▸ Predictive AI runs businesses today\n\n## Advanced Analytics &amp; Predictive AI\n\n## Business Analysis &amp; Intelligence\n\n- ▸ Foundation models provide a shortcut  for realizing the value of AI\n- ● Collecting data\n- ● Storing &amp; moving data\n- ● Transforming data\n\nData warehouses\n\n- ● Data science techniques\n- ● Predictive analytics\n- ● Real-time decision making\n\n## Big data\n\n## Foundation Models &amp; AI-enabled apps\n\n- ● Deep learning techniques\n- ● Model experimentation\n- ● Model tuning\n\n## Gen AI\n\n<!-- image -->\n\n(1)\n\n## Why is NOW  a good time to invest in AI?\n\n## Growing demand for AI solutions and services\n\n52%\n\nThe worldwide AI software market will grow to nearly $790 billion by 2026 (5 yr CAGR 18%) 1\n\nof organizations cite 'lack of MLOps tools' as a challenge 2\n\n<!-- image -->\n\nof organizations are currently investing in generative AI 3\n\n<!-- image -->\n\n## Generative AI Customer Adoption Challenges\n\n<!-- image -->\n\n<!-- image -->\n\nCost\n\nGenerative AI frontier model services are cost prohibitive at scale for most enterprise customer use cases.\n\nComplexity\n\nIntegrating models with private enterprise data for customer use cases is too difficult.\n\nFlexibility\n\nEnterprise AI use cases span data center, cloud &amp; edge and can't be constrained to a single public cloud service.\n\nVersion number here V00000\n\n<!-- image -->\n\n<!-- image -->\n\nIncrease efficiency with fast, flexible and efficient inferencing\n\nAccelerate Agentic AI delivery and stay at the forefront of innovation\n\n## Accelerate the development and delivery of AI solutions\n\n## across hybrid-cloud environments\n\nSimplified and consistent experience for connecting models to data\n\nFlexibility and consistency when scaling AI across the hybrid cloud\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nPhysical intel\n\n<!-- image -->\n\nHardware Acceleration\n\n<!-- image -->\n\nVirtual\n\n<!-- image -->\n\nPrivate\n\nCloud\n\n<!-- image -->\n\nPublic\n\nCloud\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nEdge\n\n<!-- image -->\n\n<!-- image -->\n\n## Gen AI model inference\n\n- ▸ Packaging: Linux container\n- ▸ Red Hat vLLM inference server\n- ▸ Validated &amp; optimized model repository\n- ▸ LLM Compressor tool\n- ▸ Certified: RHEL/RHEL AI and OpenShift/OpenShift AI\n- ▸ 3rd Party Support Policy: Non-Red Hat Linux &amp; Kubernetes platforms\n\nFor customers who need Gen AI model Inference on RHEL/Linux or OpenShift/Kube\n\n<!-- image -->\n\n<!-- image -->\n\n## Gen AI model inference &amp; training\n\n- ▸ Packaging: Linux server appliance\n- ▸ Granite family models\n- ▸ InstructLab model alignment\n- ▸ Optimized RHEL image with integrated accelerators\n- ▸ Includes Red Hat AI Inference Server\n\nFor customers who need an integrated Gen AI Linux server appliance for inference &amp; training\n\n<!-- image -->\n\n## Gen AI model inference, training &amp; LLMOps\n\n- ▸ Packaging: Kubernetes distributed cluster\n- ▸ Supports Gen AI &amp; Predictive AI\n- ▸ Distributed Training, Tuning &amp; Inference\n- ▸ LLMOps &amp; MLOps / Day 2 Mgt\n- ▸ Includes RHEL AI\n- ▸ Includes Red Hat AI Inference Server\n\nFor customers who need a complete distributed Gen AI platform for inference, training and LLMOps\n\n<!-- image -->\n\n## Any model. Any accelerator. Any cloud.\n\n<!-- image -->\n\nHITACHI\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nGoogle Cloud\n\nIntel Intel\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nMistral AI\n\n<!-- image -->\n\n<!-- image -->\n\n## Areas of focus for 2025 and Beyond\n\nModels\n\nGranite Models 3rd party open models\n\nModel Validation Model Optimizations\n\nMulti-lingual; Multimodal Reasoning models\n\nData ingestion and chunking Synthetic Data Generation Training Evaluation RAG\n\n## Alignment to Enterprise AI\n\nOptimizations Service Level Objectives Massive scaling Model Hub and Registry\n\n## Model Serving\n\nAPI: Inference, datasets, safety, vector\\_io, telemetry, Agents\n\nTool/Function Calling\n\n## AI Agents and Apps\n\nLLMOps MLOps\n\nObservability: TFT, ITL, TPS, Metrics, Logs, Traces Costs: $/million i/o tokens\n\nSecurity: Provenance, Signing, Encryption Governance: guardrails\n\nHybrid Cloud\n\nHardware Accelerators, OEMs, Clouds\n\nOpen Source\n\nModels, Tools, Frameworks\n\nPartners\n\nModel Providers, OEMs, CSPs, ISVs\n\n<!-- image -->\n\n## Model Inference/Serving\n\nVersion number here V00000\n\n<!-- image -->\n\n## The power of open\n\n## There has been an explosion of capability from open-source over the last 2 years\n\n<!-- image -->\n\n<!-- image -->\n\n## vLLM Inference Server - Connecting Models to the Hardware\n\nvLLM is emerging as the defacto open platform for inferencing\n\n## Model Creators\n\n## Hardware Vendors\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Contribution Trajectory\n\nInteractive Demo: red.ht/vllm-interactive vLLM is available as GA in all the Red Hat AI platforms\n\nVersion number here V00000\n\n<!-- image -->\n\n## Unchanged\n\n- ● User-level APIs\n- ● Models\n- ● GPU Kernels\n\n## Changed\n\n- ● Scheduler\n- ● Memory Manager\n- ● Model Runner\n- ● Utility functions\n- ● API Server\n- → New implementation of speculative decoding!\n- → V1 gives better performance across the board!\n\nvLLM scheduler V1  is available  on RHAIIS 3.0, RHEL AI 1.5 and RHOAI 2.20\n\n## vLLM scheduler V1\n\nRe-architect the \"core\" of vLLM based on the lessons from V0\n\n<!-- image -->\n\n## Red Hat AI tooling for model optimization\n\nOptimize and validate your choice of model\n\n<!-- image -->\n\n<!-- image -->\n\n## Inference benchmarks with GuideLLM\n\n## Accuracy evaluation with LM-eval-harness\n\n<!-- image -->\n\n## LLM Compression tools\n\nTool for evaluating LLM performance to guarantee efficient, scalable, and affordable inference serving.\n\nFramework for reducing the size and computational requirements of a LLMs while preserving accuracy\n\nA unified framework for evaluating the accuracy of LLMs across a variety of tasks and benchmarks.\n\n## Receive tailored capacity planning guidance from our experts\n\nInteractive demo: red.ht/llm-compress-interactive\n\nA\n\nLlm-compressor and GuideLLM are Developer Preview; lm-evaluation-harness is GA\n\n<!-- image -->\n\n## Compressed DeepSeek-R1 models\n\nState-of-the-art, open-source quantized reasoning models built on the DeepSeek-R1-Distill\n\n- ▸ FP8 and INT8 quantized versions achieve near-perfect accuracy recovery  across all tested reasoning benchmarks and model sizes —except for the smallest INT8 1.5B model, which reaches 97%.\n- ▸ INT4 models recover 97%+ accuracy  for 7B and larger models, with the 1.5B model maintaining ~94%.\n- ▸ With vLLM 0.7.2, deliver 4X better inference performance across many common inference scenarios.\n- ▸ Reduce GPU requirements  by (e.g. 50% for INT8)\n\n<!-- image -->\n\n<!-- image -->\n\n## Red Hat AI repository on Hugging Face\n\nA collection of third-party validated and optimized large language models\n\n<!-- image -->\n\n<!-- image -->\n\n19\n\n## Introducing the llm-d project\n\nIndustry leaders unite to power distributed, scalable gen AI inference\n\n- ● Inference at scale everywhere: Gen AI will\n- be as ubiquitous as Linux\n- ● Powered by Kubernetes &amp; vLLM:\n- Unlocking efficient, scalable inference\n- ● Backed by industry leaders: founded in collaboration with CoreWeave, Google, IBM Research, NVIDIA\n- ○ Supported by AMD, Cisco, Intel, Lambda and Mistral AI\n\n<!-- image -->\n\n<!-- image -->\n\n## llm-d: distributed inference at scale\n\nFlexible and Distributed architecture to meet SLOs efficiently\n\n## Core Features\n\n## Benefits\n\n- ● Prefill/decode disaggregation\n- ● Maximize Token Revenue\n- ● KV Cache distribution, offloading\n- ● AI-aware router\n- ● Operational telemetry for production\n- ● vLLM and Kubernetes-based\n- ● NIXL inference transfer library\n- ● Reduce Token Costs\n- ● Boost Latency and Throughput\n- ● Seamless Scaling\n\n<!-- image -->\n\n<!-- image -->\n\n## Model Customization / Alignment\n\nVersion number here V00000\n\n<!-- image -->\n\n## Enterprises need models aligned to their private data\n\nLLMs are trained with a range of public data, not enterprise-relevant data\n\nLess than 1%  of all enterprise data is represented in foundation models\n\n<!-- image -->\n\n## Enterprise organizations need to\n\n- 1. Start from a trusted base model\n- 2. Create a new representation of their data\n- 3. Deploy, scale, and create value with their AI\n\n<!-- image -->\n\n<!-- image -->\n\n## For AI Giants, Smaller Is Sometimes Better\n\nCompanies are turning their attention to less powerful models hoping lower costs and solid performance will win more customers\n\n<!-- image -->\n\n<!-- image -->\n\n## Open Source AI and Rise of Small Language Models\n\n## Smaller models are more efficient &amp; customizable\n\n## MIT Technology Review\n\n<!-- image -->\n\n## VentureBeat\n\n## Why small language models are the next big thing in AI\n\n<!-- image -->\n\n- ● Open source  AI models (Llama, Mistral, Granite, &amp; more) are catching up to proprietary models\n- ● Small language models (SLMs)  are orders of magnitude smaller than frontier models like GPT4 (&lt;10 Billion parameters vs. &gt;1 Trillion)\n- ● Cost: Run cheaper, faster and consume less energy on less powerful hardware\n- ● Customization: Can be tuned &amp; customized with private enterprise data for domain specific tasks\n- ● Control:  Customers own their own models and can create multiple instances for different use cases and deployment environments\n\n<!-- image -->\n\n## The value of open source and smaller language models\n\n## Smaller models are more efficient &amp; customizable\n\n<!-- image -->\n\n- ▸ Open source AI models are catching up to proprietary models.\n- ▸ Smaller language models, like  IBM Granite, are orders of magnitude smaller than frontier models.\n- ■ Models with less than 10 billion parameters are cheaper and faster to run, and consume less energy.\n- ▸ These models can be tuned and customized with private enterprise data  for domain specific tasks.\n- ▸ Customers own their own models and can create multiple instances for different use cases and deployment environments.\n\n<!-- image -->\n\n## Model Alignment Approaches\n\n## RAG and Fine Tuning increases accuracy and optimizes costs\n\n<!-- image -->\n\nInstructLab provides more accessible fine tuning  &amp; complements RAG (RAFT)\n\nVersion number here V00000\n\n<!-- image -->\n\n## Aligning AI to the Enterprise\n\nRAG and Fine Tuning with Red Hat AI\n\n<!-- image -->\n\n## Distributed InstructLab\n\n## Overview\n\nModules for InstructLab model customization can run in a distributed manner for scale and resilience\n\nWhy does it matter?\n\nTo get enterprise data into models in a secure, efficient, collaborative way. New approaches are needed all of the tools of the iLab toolkit are showcased in a distributed environment like RHOAI\n\n<!-- image -->\n\n<!-- image -->\n\nRed Hat\n\nThe value of enterprise data can be seen in tuning smaller, targeted, optimized models to deliver state-of-the-art performance at considerably lower cost.\n\n<!-- image -->\n\n*SaaS cost per million tokes (assuming blend of 80% input, 20% output), https://artificialanalysis.ai/models/prompt-options/multiple/medium#pricing\n\nInnovation's from AI researchers at Red Hat and the AI community\n\nProduct-driven. Open-source AI.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Inference-time scaling\n\nFrom throughput to accuracy — redefining inference for real-world tasks.\n\nDr. Sow , Particle Filtering , SQuat\n\n## Customization of Reasoning Models\n\nTurning enterprise knowledge into reasoning power.\n\nReasoning Blog , Async-GRPO\n\n## Customization of Instruct Models\n\nAdapting instruction-tuned models without forgetting what matters.\n\nSculpting Subspaces\n\n<!-- image -->\n\n## AI Agents\n\nVersion number here V00000\n\n<!-- image -->\n\n## AI agents integrate models, functions &amp; tools\n\nGen AI Models, Predictive AI Models, Code Functions, Search &amp; more\n\n<!-- image -->\n\n<!-- image -->\n\n## Red Hat AI provides an agile, stable foundation to accelerate the development and deployment of AI agentic workflows.\n\n- ▸ Allows running and managing agents as microservices.\n- ▸ Simplifies production deployment by managing LLM serving and scaling.\n- ▸ Offers native capabilities to build and manage agents with Llama Stack, and standardized communication protocols (MCP).\n- ▸ Provides the flexibility to integrate preferred tools like LangChain and Crew AI.\n\n<!-- image -->\n\n<!-- image -->\n\n## A modular approach to building AI agents\n\n<!-- image -->\n\n## Hardware accelerators\n\nDeploy anywhere\n\n## Red Hat AI allows to:\n\n- ▸ Build agents using Llama Stack's native capabilities and implementations .\n- ▸ Bring compatible Llama Stack implementations to OpenShift AI.\n- ▸ Use your own agent framework k and selectively incorporate Llama Stack APIs.\n- ▸ Build with Core Primitives  and manage your own agent framework as a standard workloads.\n\n<!-- image -->\n\n## AI Platforms\n\nVersion number here V00000\n\n<!-- image -->\n\n<!-- image -->\n\n## Generative AI, Predictive AI &amp; MLOps capabilities for building flexible, trusted AI solutions at scale\n\n<!-- image -->\n\n|              | Model Development & Tuning                                                                                                                                                                                                                                                                                                                                                                        | Model Serving                                                                                                                                                                                                                                                                                                                                                                                     | Data & model pipelines                                                                                                                                                                                                                                                                                                                                                                            | Model Monitoring                                                                                                                                                                                                                                                                                                                                                                                  |\n|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| OpenShift Al | • Workbenches with Jupyter Notebooks  as a service • Distributed training & tuning - Kueue &  Ray • ISV images | Custom images • GPU support  • Automated data science  pipeline based on Kubeflow  Pipelines  • Visual editor  • Model Registry • Serving runtimes : vLLM, other  runtimes (Custom, OVMS) • Serving engines: Kserve • Performance metrics • Operations metrics • Quality metrics | • Workbenches with Jupyter Notebooks  as a service • Distributed training & tuning - Kueue &  Ray • ISV images | Custom images • GPU support  • Automated data science  pipeline based on Kubeflow  Pipelines  • Visual editor  • Model Registry • Serving runtimes : vLLM, other  runtimes (Custom, OVMS) • Serving engines: Kserve • Performance metrics • Operations metrics • Quality metrics | • Workbenches with Jupyter Notebooks  as a service • Distributed training & tuning - Kueue &  Ray • ISV images | Custom images • GPU support  • Automated data science  pipeline based on Kubeflow  Pipelines  • Visual editor  • Model Registry • Serving runtimes : vLLM, other  runtimes (Custom, OVMS) • Serving engines: Kserve • Performance metrics • Operations metrics • Quality metrics | • Workbenches with Jupyter Notebooks  as a service • Distributed training & tuning - Kueue &  Ray • ISV images | Custom images • GPU support  • Automated data science  pipeline based on Kubeflow  Pipelines  • Visual editor  • Model Registry • Serving runtimes : vLLM, other  runtimes (Custom, OVMS) • Serving engines: Kserve • Performance metrics • Operations metrics • Quality metrics |\n|              | InstructLab Data Ingestion | Synthetic Data Generation | Phase Training | Serving | Evaluation | RAG | Agents                                                                                                                                                                                                                                                                                     | InstructLab Data Ingestion | Synthetic Data Generation | Phase Training | Serving | Evaluation | RAG | Agents                                                                                                                                                                                                                                                                                     | InstructLab Data Ingestion | Synthetic Data Generation | Phase Training | Serving | Evaluation | RAG | Agents                                                                                                                                                                                                                                                                                     | InstructLab Data Ingestion | Synthetic Data Generation | Phase Training | Serving | Evaluation | RAG | Agents                                                                                                                                                                                                                                                                                     |\n|              | vLLM                                                                                                                                                                                                                                                                                                                                                                                              | vLLM                                                                                                                                                                                                                                                                                                                                                                                              | vLLM                                                                                                                                                                                                                                                                                                                                                                                              | vLLM                                                                                                                                                                                                                                                                                                                                                                                              |\n|              |                                                                                                                                                                                                                                                                                                                                                                                                   | Pytorch| FSDP | NVIDIA CUDA | AMD ROCm | Intel Gaudi | Google TPU                                                                                                                                                                                                                                                                                                                                 | Pytorch| FSDP | NVIDIA CUDA | AMD ROCm | Intel Gaudi | Google TPU                                                                                                                                                                                                                                                                                                                                 | Pytorch| FSDP | NVIDIA CUDA | AMD ROCm | Intel Gaudi | Google TPU                                                                                                                                                                                                                                                                                                                                 |\n\nVersion number here V00000\n\n<!-- image -->\n\n## Integrated ISVs\n\n## Red Hat’s AI Partner Ecosystem\n\nAI and general ISVs\n\n## Hardware\n\n<!-- image -->\n\nDelivery partners\n\nCloud partners\n\n<!-- image -->\n\n## Conceptual machine learning architecture\n\n<!-- image -->\n\n## U.S. Department of Veterans Affairs\n\nSuicide has no single cause, and no single strategy can end this complex problem. That's why Mission Daybreak is fostering solutions across a broad spectrum of focus areas.\n\nA diversity of solutions will only be possible if a diversity of solvers answer the call to collaborate and share their expertise.\n\n## Red Hat, Team Guidehouse named winner in Mission Daybreak challenge to reduce Veteran suicides\n\n## Challenge\n\nDevelop new data-driven means of identifying Veterans at risk for suicide.\n\n## Solution\n\nRed Hat teamed with global consulting services provider Guidehouse and Philip Held, Ph.D. of Rush University Medical Center, to develop a new data-driven means of identifying Veterans at risk for suicide running on Red Hat OpenShift, leveraging Red Hat OpenShift API Management  and Red Hat OpenShift AI .\n\n## Results\n\n- ● Named a winner in the Mission Daybreak challenge, Phase 2, of the U.S. Department of Veterans Affairs' (VA) Mission Daybreak Grand Challenge in support of cutting-edge suicide prevention solutions\n- ● Moved forward with a solution for the VA's efforts to reduce Veteran suicides\n- ● Showcased the repeatability and scalability of open source-enabled solutions\n\n<!-- image -->\n\n## The Red Hat AI Platform\n\n<!-- image -->\n\n## Guardrails for Generative AI in Red Hat AI\n\n- ▸ Ensure secure, compliant, and efficient  AI operations with These key features:\n- ･ Customizable Input and Output Validators: Tailor the AI's behavior to meet your business needs\n- ･ Request-Time Configuration: Dynamically apply guardrails on a per-request basis\n- ･ Role-Specific Detection: Design targeted validation pathways for different user groups\n- ▸ Protect customer's Brand: Prevent mentions of competitors to maintain focus on your products.\n- ▸ Minimize Risk: Restrict contract creation and negotiation to human oversight.\n- ▸ Enhance Customer Experience: Provide role-specific, meaningful interactions for every user group.\n\n<!-- image -->\n\nPS: signing and securing model artifacts is part of the Model Registry’s OCI-compliance storage provided by OpenShift AI\n\n## Before MaaS\n\n<!-- image -->\n\n## After MaaS\n\n<!-- image -->\n\n<!-- image -->\n\n## Models as a Service ( MaaS )\n\nOffering AI models as the  service  to a larger audience\n\n- ● IT serves common models centrally\n- ○ Generative AI focus, applicable to any model\n- ○ Centralized pool of hardware\n- ○ Platform Engineering for AI\n- ○ AI management (versioning, regression testing, etc)\n- ● Models available through API Gateway\n- ● Developers consume models, build AI applications\n- ○ For end users (private assistants, etc)\n- ○ To improve products or services through AI\n- ● Shared Resources business model keeps costs down\n\n<!-- image -->\n\n## GPUaaS\n\n- ▸ Enables efficient management and allocation of GPU resources for a variety of AI workloads: workbenches, training/tuning jobs, model serving\n- ▸ Supports both whole and fractional GPU allocation\n- ▸ Includes observability tools for resource optimization and to facilitate chargeback scenarios\n\n<!-- image -->\n\n## Why does it matter?\n\n- -Improving Resource Utilization: Reclaiming idle GPUs and optimizing allocation to reduce waste\n- -Supporting the complete AI Lifecycle: Handling workloads from notebooks to model serving\n- -Providing Visibility: Offering metrics for both data scientists and administrators\n\n<!-- image -->\n\n<!-- image -->\n\n## Thank you\n\n<!-- image -->\n\nlinkedin.com/company/red-hat youtube.com/user/RedHatVideos\n\n<!-- image -->\n\n<!-- image -->\n\nfacebook.com/redhatinc twitter.com/RedHat\n\n<!-- image -->\n\n<!-- image -->\n\n## Model as a Service\n\nVersion number here V00000\n\n<!-- image -->\n\nCPUs &amp; especially GPUs\n\n## Infrastructure as a Service  can be costly\n\n<!-- image -->\n\n<!-- image -->\n\nSelf-Service is good for plentiful resources &amp; small teams\n\n- ● Throwing GPUs at the problem is risky\n- ● Few people know how to use them correctly\n- ● Leads to duplication and underutilization\n- ● Leads to high costs\n- ● Most people want an LLM endpoint, not a GPU\n\n<!-- image -->\n\nCONFIDENTIAL Red Hat Associate and\n\n## Models as a Service ( MaaS )\n\nNDA partner use only, no further distribution\n\nOffering AI models as the  service  to a larger audience\n\n- ● IT serves common models centrally\n- ○ Generative AI focus, applicable to any model\n- ○ Centralized pool of hardware\n- ○ Platform Engineering for AI\n- ○ AI management (versioning, regression testing, etc)\n- ● Models available through API Gateway\n- ● Developers consume models, build AI applications\n- ○ For end users (private assistants, etc)\n- ○ To improve products or services through AI\n- ● Shared Resources business model keeps costs down\n\n<!-- image -->\n\nRed Hat One 2025\n\n## Hosted  AI services are not  the only option\n\n<!-- image -->\n\nBecome the Private AI Provider\n\n<!-- image -->\n\n## Risks &amp; Challenges:\n\n- ● Costs at scale\n- ● Data privacy and security policies\n- ● IP leakage\n\n<!-- image -->\n\n<!-- image -->\n\nCONFIDENTIAL Red Hat Associate and\n\nNDA partner use only, no further distribution\n\n## Example: Private Customer Service\n\n## knowledge application\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Today's  infrastructure + tomorrow's strategy\n\n<!-- image -->\n\n<!-- image -->\n\n## What’s not new:\n\n## What’s new:\n\n- ● IT manages AI infrastructure\n- ● Platform, hardware &amp; access centrally located\n- ● Data Scientists use GPUs to customize models\n- ● IT &amp; Data Scientists monitor &amp; evaluate performance\n- ● Models served to a wider audience\n- ● IT adds API Gateway for production serving\n- ● Developers build using standardized endpoints\n- ● Associates consume Private AI services\n\n<!-- image -->\n\n<!-- image -->\n\n## Stable foundation for expanding use\n\nCONFIDENTIAL Red Hat Associate and\n\nNDA partner use only, no further distribution\n\n## cases and techniques\n\n<!-- image -->\n\n## Success generates success:\n\n## RAG, FT, &amp; RAFT in simple terms:\n\n- ● Add models for new use cases and applications\n- ● Data Scientists build specialized models\n- ● Use RAG, FT, or RAFT for improved model results\n- ● Expand proven scalable environment\n- ● RAG - supplement model's basic information with details\n- ● FT - train model a bit more with detailed information\n- ● RAFT - combines the two, retrieve details and train\n- * Most organizations progress in this order\n\n<!-- image -->\n\n## Become the Private AI Provider\n\n## for your organization\n\n## What is Models as a Service\n\n## Use cases\n\n- ● Strategy delivering central AI services privately\n- ● Model service consumed by large audience\n- ● Accessible to Developers and Associates\n- ● GPUs invisible to user, critical for cost optimization\n\n## Why IT should become the Private AI Provider\n\n- ● Compliant with existing security, data &amp; privacy policies\n- ● Predictable costs &amp; increased utilization\n- ● Reduce time to market with AI applications\n- ● Unified &amp; impactful service delivery\n\n## How value is created\n\n- ● AI managed like any other workload\n- ● Innovation across entire organization\n- ● Plug into existing cost models\n- ● Reduced costs, risk, and overhead\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nEmbedding\n\nApplication\n\n<!-- image -->\n\n<!-- image -->\n\nYour next app\n\n<!-- image -->\n\nCONFIDENTIAL Red Hat Associate and\n\nNDA partner use only, no further distribution\n\n## Red Hat can help\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nRed Hat OpenShift\n\nRedHat Consulting\n\n<!-- image -->\n\n## Before MaaS\n\n<!-- image -->\n\n## After MaaS\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Before  MaaS\n\n- ● Self-Service access to GPUs  can lead to great inefficiencies:\n- ○ Duplication of models\n- ○ Duplication of efforts\n- ○ Lack of accountability\n- ○ Low GPU utilization\n- ○ Unnecessarily high costs\n\n<!-- image -->\n\n<!-- image -->\n\n## After  MaaS\n\n- ● STOP providing access to GPUs .\n- ● Instead:\n- ● Offer self-service access to Models\n- ● Leads to great benefits:\n- ○ High traceability\n- ○ Most people would rather use the models (than the GPUs)\n- ○ Lower TCO / chargeback, etc.\n- ○ Increase utilization\n\n<!-- image -->\n\n## MaaS Principles\n\n- ● Become the provider of Private AI:\n- ○ Don't just \"throw GPUs at the problem\"\n- ○ Team of experts serve each model only once\n- ○ Provide self-service access to the models\n- ○ API gateway to track model consumption\n\n<!-- image -->\n\n## MaaS Principles\n\n- ● Internally replicate the business structure of public AI providers (like Gemini, OpenAI, etc…)\n- ○ None of them give you access to their GPUs\n\n<!-- image -->\n\nReadily available models for your Developers\n\n- ● Getting access to Private AI models becomes as trivial as using OpenAI/Gemini/Etc..\n\n<!-- image -->\n\n## MaaS is your engine for \"AI for all\"\n\n- ● Your devs can now easily build AI-Powered applications\n- ● And now, your employees and customers can easily leverage AI-powered applications\n\n<!-- image -->\n\n60\n\n## But who's keeping track?\n\n- ● No such thing as a free AI lunch\n- ○ But MaaS will minimize the costs\n- ○ then further reduce costs, by using quantized models\n- ● Manage your models with GitOps\n- ● API gateway keeps tracks of hits and tokens\n- ● Chargeback proportional to usage/tokens\n\n<!-- image -->\n\n61\n\n## Centrally and reliably manage your models via GitOps\n\n<!-- image -->\n\n<!-- image -->\n\n62\n\n## Centrally and reliably manage your models via GitOps\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Centrally and reliably manage your models via GitOps\n\n<!-- image -->\n\n## Your API Gateway tracks things\n\n<!-- image -->\n\n<!-- image -->\n\n## You can see how many \"Apps\" your developers have created\n\n<!-- image -->\n\n<!-- image -->\n\n## And the details for one user\n\n<!-- image -->\n\n<!-- image -->\n\n## Track the usage of one model over time\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Number of Hits over last 24H\n\n<!-- image -->\n\n<!-- image -->\n\n## Number of Tokens over 24H\n\n<!-- image -->\n\n<!-- image -->\n\n## And over 30 days\n\n<!-- image -->\n\n<!-- image -->\n\n## But who's using this model the most?\n\n<!-- image -->\n\n<!-- image -->\n\n72\n\n<!-- image -->\n\n## Integrated AI platform\n\nCreate and deliver GenAI and predictive AI  models at scale across hybrid cloud environments.\n\n## Available as\n\n- ● Fully managed cloud service\n- ● Traditional software product on-site or in the cloud!\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Model development\n\nBring your own models  or customize Granite models to your use case with your data. Supports integration of multiple AI/ML libraries, frameworks, and runtimes.\n\n## Model serving and monitoring\n\nDeploy models across any OpenShift footprint  and centrally monitor their performance.\n\n## Lifecycle management\n\nExpand DevOps practices to MLOps  to manage the entire AI/ML lifecycle .\n\n## Resource optimization and management\n\nScale  to meet workload demands of GenAI and predictive\n\nAI models.  Share resources, projects, and models across environments.\n\n<!-- image -->\n\n## OpenShift AI Components\n\n<!-- image -->\n\n<!-- image -->\n\n## Hybrid cloud deployment for AI\n\nAcross different hardware accelerators, on-prem OEM servers, and cloud environments\n\nHardware\n\nCloud\n\nAccelerators\n\nOEM Servers\n\nEnvironments\n\n<!-- image -->\n\n<!-- image -->\n\n## Model Catalog\n\nOpenShift AI users now have the ability to access Red Hat and 3rd party models and easily deploy them from within the application.\n\n<!-- image -->\n\n<!-- image -->",
  "metadata": {
    "filename": "Red Hat AI_ Our vision, strategy and roadmap- RTX Corporation.pdf",
    "format": "markdown",
    "processed_at": "2025-07-28T23:41:35.580691",
    "pages": 0,
    "tables": 0,
    "formulas": 0,
    "endpoint_used": "https://docling-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1alpha/convert/source",
    "processing_time": 158.0293492130004,
    "method": "Docling_API"
  },
  "processed_at": "2025-07-28T23:41:35.588030",
  "success": true
}