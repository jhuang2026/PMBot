{
  "file_path": "documents/inference_server/Red_Hat_AI_Inference_Server-3.2-Supported_product_and_hardware_configurations-en-US.pdf",
  "product": "inference_server",
  "product_name": "Red Hat AI Inference Server",
  "filename": "Red_Hat_AI_Inference_Server-3.2-Supported_product_and_hardware_configurations-en-US.pdf",
  "content": "Red Hat AI Inference Server\n \n3.2\nSupported product and hardware\nconfigurations\nSupported hardware and software configurations for deploying Red Hat AI Inference\nServer\nLast Updated: 2025-07-22\n\nRed Hat AI Inference Server\n \n3.2\n \nSupported product and hardware\nconfigurations\nSupported hardware and software configurations for deploying Red Hat AI Inference Server\nLegal Notice\nCopyright \n©\n 2025 Red Hat, Inc.\nThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons\nAttribution–Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is\navailable at\nhttp://creativecommons.org/licenses/by-sa/3.0/\n. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must\nprovide the URL for the original version.\nRed Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert,\nSection 4d of CC-BY-SA to the fullest extent permitted by applicable law.\nRed Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift,\nFedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States\nand other countries.\nLinux ®\n is the registered trademark of Linus Torvalds in the United States and other countries.\nJava ®\n is a registered trademark of Oracle and/or its affiliates.\nXFS ®\n is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States\nand/or other countries.\nMySQL ®\n is a registered trademark of MySQL AB in the United States, the European Union and\nother countries.\nNode.js ®\n is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the\nofficial Joyent Node.js open source or commercial project.\nThe \nOpenStack ®\n Word Mark and OpenStack logo are either registered trademarks/service marks\nor trademarks/service marks of the OpenStack Foundation, in the United States and other\ncountries and are used with the OpenStack Foundation's permission. We are not affiliated with,\nendorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\nAll other trademarks are the property of their respective owners.\nAbstract\nLearn about supported hardware and software configurations for Red Hat AI Inference Server.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nTable of Contents\nPREFACE\nCHAPTER 1. PRODUCT AND VERSION COMPATIBILITY\nCHAPTER 2. SUPPORTED AI ACCELERATORS\nCHAPTER 3. SUPPORTED DEPLOYMENT ENVIRONMENTS\nCHAPTER 4. OPENSHIFT CONTAINER PLATFORM SOFTWARE PREREQUISITES FOR GPU DEPLOYMENTS\nCHAPTER 5. LIFECYCLE AND UPDATE POLICY\n3\n4\n5\n7\n8\n9\nTable of Contents\n1\nRed Hat AI Inference Server 3.2 Supported product and hardware configurations\n2\nPREFACE\nThis document describes the supported hardware, software, and delivery platforms that you can use to\nrun Red Hat AI Inference Server in production environments.\nIMPORTANT\nTechnology Preview\n and \nDeveloper Preview\n features are provided for early access to\npotential new features.\nTechnology Preview or Developer Preview features are not supported or recommended\nfor production workloads.\nAdditional resources\nRed Hat AI Inference Server documentation\nRed Hat AI on Hugging Face\nLLM Compressor techniques\nPREFACE\n3\nCHAPTER 1. PRODUCT AND VERSION COMPATIBILITY\nThe following table lists the supported product versions for Red Hat AI Inference Server 3.2.\nTable 1.1. Product and version compatibility\nProduct\nSupported version\nRed Hat AI Inference Server\n3.2\nvLLM core\nv0.9.2\nLLM Compressor\nv0.6.0\nRed Hat AI Inference Server 3.2 Supported product and hardware configurations\n4\nCHAPTER 2. SUPPORTED AI ACCELERATORS\nThe following tables list the supported AI data center grade accelerators for Red Hat AI Inference\nServer 3.2.\nIMPORTANT\nRed Hat AI Inference Server only supports data center grade accelerators.\nRed Hat AI Inference Server 3.2 is not compatible with CUDA versions lower than 12.8.\nTable 2.1. Supported NVIDIA AI accelerators\nContainer\nimage\nvLLM release\nAI\naccelerators\nRequirements\nvLLM\narchitecture\nsupport\nLLM\nCompressor\nsupport\nrhaiis/vllm\n‑\nc\nuda-rhel9\nvLLM v0.9.2\nNVIDIA data\ncenter GPUs:\nTurin\ng: T4\nAmpe\nre: A2,\nA10,\nA16,\nA30,\nA40,\nA100\nAda:\nL4,\nL40,\nL40S\nHopp\ner:\nH100,\nH200,\nGH20\n0\nBlack\nwell:\nB200,\nRTX\nPRO\n6000\nBlack\nwell\nServe\nr\nEditio\nn\nCUDA\nToolki\nt 12.8\nNVIDI\nA\nConta\niner\nToolki\nt 1.14\nNVIDI\nA\nGPU\nOpera\ntor\n24.3\nPytho\nn 3.12\nPyTor\nch\n2.7.0\nx86\nAarch\n64\nDevel\noper\nPrevi\new\nx86\nTechnology\nPreview\nNOTE\nCHAPTER 2. SUPPORTED AI ACCELERATORS\n5\nNOTE\nNVIDIA T4 and A100 accelerators do not support FP8 (W8A8) quantization.\nTable 2.2. Supported AMD AI accelerators\nContainer\nimage\nvLLM release\nAI\naccelerators\nRequirements\nvLLM\narchitecture\nsupport\nLLM\nCompressor\nsupport\nrhaiis/vllm\n‑\nr\nocm-rhel9\nvLLM v0.9.2\nAMD\nInstin\nct\nMI210\nAMD\nInstin\nct\nMI30\n0X\nROC\nm 6.2\nAMD\nGPU\nOpera\ntor\n6.2\nPytho\nn 3.12\nPyTor\nch\n2.7.0\nx86\nx86\nTechnology\nPreview\nNOTE\nAMD GPUs support FP8 (W8A8) and GGUF quantization schemes only.\nTable 2.3. Google TPU AI accelerators (Developer Preview)\nContainer\nimage\nvLLM release\nAI\naccelerators\nRequirements\nvLLM\narchitecture\nsupport\nLLM\nCompressor\nsupport\nrhaiis/vllm\n‑\nxl\na-rhel9\nvLLM v0.8.5\nGoogle TPU\nv6e\nPytho\nn 3.12\nPyTor\nch\n2.7.0\nx86 Developer\nPreview\nNot supported\nRed Hat AI Inference Server 3.2 Supported product and hardware configurations\n6\nCHAPTER 3. SUPPORTED DEPLOYMENT ENVIRONMENTS\nThe following deployment environments for Red Hat AI Inference Server are supported.\nTable 3.1. Red Hat AI Inference Server supported deployment environments\nEnvironment\nSupported versions\nDeployment notes\nOpenShift\nContainer\nPlatform\n(self\n‑\nmanaged)\n4.14 – 4.19\nDeploy on bare\n‑\nmetal hosts or virtual machines.\nRed Hat\nOpenShift\nService on\nAWS (ROSA)\n4.14 – 4.19\nRequires \nROSA STS cluster\n with GPU\n‑\nenabled P5 or\nG5 node types.\nRed Hat\nEnterprise\nLinux (RHEL)\n9.2 – 10.0\nDeploy on bare\n‑\nmetal hosts or virtual machines.\nLinux (not\nRHEL)\n-\nSupported under third\n‑\nparty policy deployed on\nbare\n‑\nmetal hosts or virtual machines. OpenShift\nContainer Platform Operators are not required.\nKubernetes\n(not OpenShift\nContainer\nPlatform)\n-\nSupported under third\n‑\nparty policy deployed on\nbare\n‑\nmetal hosts or virtual machines.\nNOTE\nRed Hat AI Inference Server is available only as a container image. The host operating\nsystem and kernel must support the required accelerator drivers. For more information,\nsee \nSupported AI accelerators\n.\nCHAPTER 3. SUPPORTED DEPLOYMENT ENVIRONMENTS\n7\nCHAPTER 4. OPENSHIFT CONTAINER PLATFORM SOFTWARE\nPREREQUISITES FOR GPU DEPLOYMENTS\nThe following table lists the OpenShift Container Platform software prerequisites for GPU deployments.\nTable 4.1. Software prerequisites for GPU deployments\nComponent\nMinimum version\nOperator\nNVIDIA GPU Operator\n24.3\nNVIDIA GPU Operator OLM\nOperator\nAMD GPU Operator\n6.2\nAMD GPU Operator OLM\nOperator\nNode Feature Discovery \n[1]\n4.14\nNode Feature Discovery\nOperator\n[1] Included by default with OpenShift Container Platform. Node Feature Discovery is required for\nscheduling NUMA-aware workloads\n.\nRed Hat AI Inference Server 3.2 Supported product and hardware configurations\n8\nCHAPTER 5. LIFECYCLE AND UPDATE POLICY\nSecurity and critical bug fixes are delivered as container images available from the \nregistry.access.redhat.com/rhaiis\n container registry and are announced through RHSA advisories.\nSee \nRHAIIS container images on catalog.redhat.com\n for more details.\nCHAPTER 5. LIFECYCLE AND UPDATE POLICY\n9\n",
  "metadata": {
    "filename": "Red_Hat_AI_Inference_Server-3.2-Supported_product_and_hardware_configurations-en-US.pdf",
    "format": "text",
    "processed_at": "2025-07-28T23:46:35.999377",
    "pages": 13,
    "method": "PyPDF2_fallback"
  },
  "processed_at": "2025-07-28T23:46:35.999449",
  "success": true
}