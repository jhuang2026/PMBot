{
  "file_path": "documents/inference_server/Red_Hat_AI_Inference_Server-3.2-Getting_started-en-US.pdf",
  "product": "inference_server",
  "product_name": "Red Hat AI Inference Server",
  "filename": "Red_Hat_AI_Inference_Server-3.2-Getting_started-en-US.pdf",
  "content": "Red Hat AI Inference Server\n \n3.2\nGetting started\nGetting started with Red Hat AI Inference Server\nLast Updated: 2025-07-22\n\nRed Hat AI Inference Server\n \n3.2\n \nGetting started\nGetting started with Red Hat AI Inference Server\nLegal Notice\nCopyright \n©\n 2025 Red Hat, Inc.\nThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons\nAttribution–Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is\navailable at\nhttp://creativecommons.org/licenses/by-sa/3.0/\n. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must\nprovide the URL for the original version.\nRed Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert,\nSection 4d of CC-BY-SA to the fullest extent permitted by applicable law.\nRed Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift,\nFedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States\nand other countries.\nLinux ®\n is the registered trademark of Linus Torvalds in the United States and other countries.\nJava ®\n is a registered trademark of Oracle and/or its affiliates.\nXFS ®\n is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States\nand/or other countries.\nMySQL ®\n is a registered trademark of MySQL AB in the United States, the European Union and\nother countries.\nNode.js ®\n is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the\nofficial Joyent Node.js open source or commercial project.\nThe \nOpenStack ®\n Word Mark and OpenStack logo are either registered trademarks/service marks\nor trademarks/service marks of the OpenStack Foundation, in the United States and other\ncountries and are used with the OpenStack Foundation's permission. We are not affiliated with,\nendorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\nAll other trademarks are the property of their respective owners.\nAbstract\nLearn how to work with Red Hat AI Inference Server for model serving and inferencing.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nTable of Contents\nPREFACE\nCHAPTER 1. ABOUT AI INFERENCE SERVER\nCHAPTER 2. PRODUCT AND VERSION COMPATIBILITY\nCHAPTER 3. SERVING AND INFERENCING WITH AI INFERENCE SERVER\nCHAPTER 4. VALIDATING RED HAT AI INFERENCE SERVER BENEFITS USING KEY METRICS\nCHAPTER 5. TROUBLESHOOTING\n5.1. MODEL LOADING ERRORS\n5.2. MEMORY OPTIMIZATION\n5.3. GENERATED MODEL RESPONSE QUALITY\n5.4. CUDA ACCELERATOR ERRORS\n5.5. NETWORKING ERRORS\n5.6. PYTHON MULTIPROCESSING ERRORS\n5.7. GPU DRIVER OR DEVICE PASS-THROUGH ISSUES\n3\n4\n5\n6\n11\n13\n13\n15\n15\n16\n16\n17\n17\nTable of Contents\n1\nRed Hat AI Inference Server 3.2 Getting started\n2\nPREFACE\nRed Hat AI Inference Server is a container image that optimizes serving and inferencing with LLMs.\nUsing AI Inference Server, you can serve and inference models in a way that boosts their performance\nwhile reducing their costs.\nPREFACE\n3\nCHAPTER 1. ABOUT AI INFERENCE SERVER\nAI Inference Server provides enterprise-grade stability and security, building on upstream, open source\nsoftware. AI Inference Server leverages the upstream \nvLLM project\n, which provides state-of-the-art\ninferencing features.\nFor example, AI Inference Server uses continuous batching to process requests as they arrive instead of\nwaiting for a full batch to be accumulated. It also uses tensor parallelism to distribute LLM workloads\nacross multiple GPUs. These features provide reduced latency and higher throughput.\nTo reduce the cost of inferencing models, AI Inference Server uses paged attention. LLMs use a\nmechanism called attention to understand conversations with users. Normally, attention uses a\nsignificant amount of memory, much of which is wasted. Paged attention addresses this memory\nwastage by provisioning memory for LLMs similar to the way that virtual memory works for operating\nsystems. This approach consumes less memory, which lowers costs.\nTo verify cost savings and performance gains with AI Inference Server, complete the following\nprocedures:\n1\n. \nServing and inferencing with AI Inference Server\n2\n. \nValidating Red Hat AI Inference Server benefits using key metrics\nRed Hat AI Inference Server 3.2 Getting started\n4\nCHAPTER 2. PRODUCT AND VERSION COMPATIBILITY\nThe following table lists the supported product versions for Red Hat AI Inference Server 3.2.\nTable 2.1. Product and version compatibility\nProduct\nSupported version\nRed Hat AI Inference Server\n3.2\nvLLM core\nv0.9.2\nLLM Compressor\nv0.6.0\nCHAPTER 2. PRODUCT AND VERSION COMPATIBILITY\n5\nCHAPTER 3. SERVING AND INFERENCING WITH AI\nINFERENCE SERVER\nServe and inference a large language model with Red Hat AI Inference Server.\nPrerequisites\nYou have installed Podman or Docker\nYou have access to a Linux server with NVIDIA or AMD GPUs and are logged in as a user with\nroot privileges\nFor NVIDIA GPUs:\nInstall NVIDIA drivers\nInstall the NVIDIA Container Toolkit\nIf your system has multiple NVIDIA GPUs that use NVswitch, you must have root access\nto start Fabric Manager\nFor AMD GPUs:\nInstall ROCm software\nVerify that you can run ROCm containers\nYou have access to \nregistry.redhat.io\n and have logged in\nYou have a Hugging Face account and have generated a Hugging Face token\nFor more information about supported quantization schemes for accelerators, see \nSupported hardware\n.\nProcedure\n1\n. \nIdentify the correct image for your infrastructure.\nAI Accelerator\nAI Inference Server image\nNVIDIA CUDA\nregistry.redhat.io/rhaiis/vllm-cuda-\nrhel9:3.2.0\nAMD ROCm\nregistry.redhat.io/rhaiis/vllm-rocm-\nrhel9:3.2.0\n2\n. \nOpen a terminal on your server host, and log in to \nregistry.redhat.io\n:\n3\n. \nPull the relevant image for your GPUs:\n$ podman login registry.redhat.io\n$ podman pull registry.redhat.io/rhaiis/vllm-<gpu_type>-rhel9:3.2.0\nRed Hat AI Inference Server 3.2 Getting started\n6\n4\n. \nIf your system has SELinux enabled, configure SELinux to allow device access:\n5\n. \nCreate a volume and mount it into the container. Adjust the container permissions so that the\ncontainer can use it.\n6\n. \nCreate or append your \nHF_TOKEN\n Hugging Face token to the \nprivate.env\n file. Source the \nprivate.env\n file.\n7\n. \nStart the AI Inference Server container image.\na\n. \nFor NVIDIA CUDA accelerators:\ni\n. \nIf the host system has multiple GPUs and uses NVSwitch, then start NVIDIA Fabric\nManager. To detect if your system is using NVSwitch, first check if files are present in \n/proc/driver/nvidia-nvswitch/devices/\n, and then start NVIDIA Fabric Manager. Starting\nNVIDIA Fabric Manager requires root privileges.\nExample output\nIMPORTANT\nNVIDIA Fabric Manager is only required on systems with multiple GPUs\nthat use NVswitch. For more information, see \nNVIDIA Server\nArchitectures\n.\nii\n. \nCheck that the Red Hat AI Inference Server container can access NVIDIA GPUs on the\nhost by running the following command:\nExample output\n$ sudo setsebool -P container_use_devices 1\n$ mkdir -p rhaiis-cache\n$ chmod g+rwX rhaiis-cache\n$ echo \"export HF_TOKEN=<your_HF_token>\" > private.env\n$ source private.env\n$ ls /proc/driver/nvidia-nvswitch/devices/\n0000:0c:09.0  0000:0c:0a.0  0000:0c:0b.0  0000:0c:0c.0  0000:0c:0d.0  0000:0c:0e.0\n$ systemctl start nvidia-fabricmanager\n$ podman run --rm -it \\\n--security-opt=label=disable \\\n--device nvidia.com/gpu=all \\\nnvcr.io/nvidia/cuda:12.4.1-base-ubi9 \\\nnvidia-smi\nCHAPTER 3. SERVING AND INFERENCING WITH AI INFERENCE SERVER\n7\n1\n2\n3\n4\niii\n. \nStart the container.\nRequired for systems where SELinux is enabled. \n--security-opt=label=disable\nprevents SELinux from relabeling files in the volume mount. If you choose not to\nuse this argument, your container might not successfully run.\nIf you experience an issue with shared memory, increase \n--shm-size\n to \n8GB\n.\nMaps the host UID to the effective UID of the vLLM process in the container. You\ncan also pass \n--user=0\n, but this less secure than the \n--userns\n option. Setting \n--\nuser=0\n runs vLLM as root inside the container.\nSet and export \nHF_TOKEN\n with your \nHugging Face API access token\nRequired for systems where SELinux is enabled. On Debian or Ubuntu operating\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8\n     \n|\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute\n \nM. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+=\n=====================|\n|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:08:01.0 Off |                    0 |\n| N/A   32C    P0             64W /  400W |       1MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA A100-SXM4-80GB          Off |   00000000:08:02.0 Off |                    0 |\n| N/A   29C    P0             63W /  400W |       1MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|====================================================================\n=====================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n$ podman run --rm -it \\\n--device nvidia.com/gpu=all \\\n--security-opt=label=disable \\ \n1\n--shm-size=4g -p 8000:8000 \\ \n2\n--userns=keep-id:uid=1001 \\ \n3\n--env \"HUGGING_FACE_HUB_TOKEN=$HF_TOKEN\" \\ \n4\n--env \"HF_HUB_OFFLINE=0\" \\\n--env=VLLM_NO_USAGE_STATS=1 \\\n-v ./rhaiis-cache:/opt/app-root/src/.cache:Z \\ \n5\nregistry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0 \\\n--model RedHatAI/Llama-3.2-1B-Instruct-FP8 \\\n--tensor-parallel-size 2 \n6\nRed Hat AI Inference Server 3.2 Getting started\n8\n5\n6\n1\n1\n2\n3\nRequired for systems where SELinux is enabled. On Debian or Ubuntu operating\nsystems, or when using Docker without SELinux, the \n:Z\n suffix is not available.\nSet \n--tensor-parallel-size\n to match the number of GPUs when running the AI\nInference Server container on multiple GPUs.\nb\n. \nFor AMD ROCm accelerators:\ni\n. \nUse \namd-smi static -a\n to verify that the container can access the host system GPUs:\nYou must belong to both the video and render groups on AMD systems to use the\nGPUs. To access GPUs, you must pass the \n--group-add=keep-groups\nsupplementary groups option into the container.\nii\n. \nStart the container:\n--security-opt=label=disable\n prevents SELinux from relabeling files in the volume\nmount. If you choose not to use this argument, your container might not\nsuccessfully run.\nIf you experience an issue with shared memory, increase \n--shm-size\n to \n8GB\n.\nSet \n--tensor-parallel-size\n to match the number of GPUs when running the AI\nInference Server container on multiple GPUs.\n8\n. \nIn a separate tab in your terminal, make a request to your model with the API.\n$ podman run -ti --rm --pull=newer \\\n--security-opt=label=disable \\\n--device=/dev/kfd --device=/dev/dri \\\n--group-add keep-groups \\ \n1\n--entrypoint=\"\" \\\nregistry.redhat.io/rhaiis/vllm-rocm-rhel9:3.2.0 \\\namd-smi static -a\npodman run --rm -it \\\n--device /dev/kfd --device /dev/dri \\\n--security-opt=label=disable \\ \n1\n--group-add keep-groups \\\n--shm-size=4GB -p 8000:8000 \\ \n2\n--env \"HUGGING_FACE_HUB_TOKEN=$HF_TOKEN\" \\\n--env \"HF_HUB_OFFLINE=0\" \\\n--env=VLLM_NO_USAGE_STATS=1 \\\n-v ./rhaiis-cache:/opt/app-root/src/.cache \\\nregistry.redhat.io/rhaiis/vllm-rocm-rhel9:3.2.0 \\\n--model RedHatAI/Llama-3.2-1B-Instruct-FP8 \\\n--tensor-parallel-size 2 \n3\ncurl -X POST -H \"Content-Type: application/json\" -d '{\n    \"prompt\": \"What is the capital of France?\",\n    \"max_tokens\": 50\n}' http://<your_server_ip>:8000/v1/completions | jq\nCHAPTER 3. SERVING AND INFERENCING WITH AI INFERENCE SERVER\n9\nExample output\n{\n    \n\"id\"\n: \n\"cmpl-b84aeda1d5a4485c9cb9ed4a13072fca\"\n,\n    \n\"object\"\n: \n\"text_completion\"\n,\n    \n\"created\"\n: \n1746555421\n,\n    \n\"model\"\n: \n\"RedHatAI/Llama-3.2-1B-Instruct-FP8\"\n,\n    \n\"choices\"\n: [\n        {\n            \n\"index\"\n: \n0\n,\n            \n\"text\"\n: \n\" Paris.\\nThe capital of France is Paris.\"\n,\n            \n\"logprobs\"\n: \nnull\n,\n            \n\"finish_reason\"\n: \n\"stop\"\n,\n            \n\"stop_reason\"\n: \nnull\n,\n            \n\"prompt_logprobs\"\n: \nnull\n        }\n    ],\n    \n\"usage\"\n: {\n        \n\"prompt_tokens\"\n: \n8\n,\n        \n\"total_tokens\"\n: \n18\n,\n        \n\"completion_tokens\"\n: \n10\n,\n        \n\"prompt_tokens_details\"\n: \nnull\n    }\n}\nRed Hat AI Inference Server 3.2 Getting started\n10\nCHAPTER 4. VALIDATING RED HAT AI INFERENCE SERVER\nBENEFITS USING KEY METRICS\nUse the following metrics to evaluate the performance of the LLM model being served with AI Inference\nServer:\nTime to first token (TTFT)\n: How long does it take for the model to provide the first token of its\nresponse?\nTime per output token (TPOT)\n: How long does it take for the model to provide an output\ntoken to each user, who has sent a request?\nLatency\n: How long does it take for the model to generate a complete response?\nThroughput\n: How many output tokens can a model produce simultaneously, across all users and\nrequests?\nComplete the procedure below to run a benchmark test that shows how AI Inference Server, and other\ninference servers, perform according to these metrics.\nPrerequisites\nAI Inference Server container image\nGitHub account\nPython 3.9 or higher\nProcedure\n1\n. \nOn your host system, start an AI Inference Server container and serve a model.\n2\n. \nIn a separate terminal tab, install the benchmark tool dependencies.\n3\n. \nClone the \nvLLM Git repository\n:\n4\n. \nRun the \n./vllm/benchmarks/benchmark_serving.py\n script.\n$ podman run --rm -it --device nvidia.com/gpu=all \\\n--shm-size=4GB -p 8000:8000 \\\n--env \"HUGGING_FACE_HUB_TOKEN=$HF_TOKEN\" \\\n--env \"HF_HUB_OFFLINE=0\" \\\n-v ./rhaiis-cache:/opt/app-root/src/.cache \\\n--security-opt=label=disable \\\nregistry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0 \\\n--model RedHatAI/Llama-3.2-1B-Instruct-FP8\n$ pip install vllm pandas datasets\n$ git clone https://github.com/vllm-project/vllm.git\n$ python vllm/benchmarks/benchmark_serving.py --backend vllm --model RedHatAI/Llama-\n3.2-1B-Instruct-FP8 --num-prompts 100 --dataset-name random  --random-input 1024 --\nrandom-output 512 --port 8000\nCHAPTER 4. VALIDATING RED HAT AI INFERENCE SERVER BENEFITS USING KEY METRICS\n11\nVerification\nThe results show how AI Inference Server performs according to key server metrics:\nTry changing the parameters of this benchmark and running it again. Notice how \nvllm\n as a backend\ncompares to other options. Throughput should be consistently higher, while latency should be lower.\nOther options for \n--backend\n are: \ntgi\n, \nlmdeploy\n, \ndeepspeed-mii\n, \nopenai\n, and \nopenai-chat\nOther options for \n--dataset-name\n are: \nsharegpt\n, \nburstgpt\n, \nsonnet\n, \nrandom\n, \nhf\nAdditional resources\nvLLM documentation\nLLM Inference Performance Engineering: Best Practices\n, by Mosaic AI Research, which explains\nmetrics such as throughput and latency\n============ Serving Benchmark Result ============\nSuccessful requests:                    100\nBenchmark duration (s):                 4.61\nTotal input tokens:                     102300\nTotal generated tokens:                 40493\nRequest throughput (req/s):             21.67\nOutput token throughput (tok/s):        8775.85\nTotal Token throughput (tok/s):         30946.83\n---------------Time to First Token----------------\nMean TTFT (ms):                         193.61\nMedian TTFT (ms):                       193.82\nP99 TTFT (ms):                          303.90\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                         9.06\nMedian TPOT (ms):                       8.57\nP99 TPOT (ms):                          13.57\n---------------Inter-token Latency----------------\nMean ITL (ms):                          8.54\nMedian ITL (ms):                        8.49\nP99 ITL (ms):                           13.14\n==================================================\nRed Hat AI Inference Server 3.2 Getting started\n12\nCHAPTER 5. TROUBLESHOOTING\nThe following troubleshooting information for Red Hat AI Inference Server 3.2 describes common\nproblems related to model loading, memory, model response quality, networking, and GPU drivers.\nWhere available, workarounds for common issues are described.\nMost common issues in vLLM relate to installation, model loading, memory management, and GPU\ncommunication. Most problems can be resolved by using a correctly configured environment, ensuring\ncompatible hardware and software versions, and following the recommended configuration practices.\nIMPORTANT\nFor persistent issues, export \nVLLM_LOGGING_LEVEL=DEBUG\n to enable debug\nlogging and then check the logs.\n5.1. MODEL LOADING ERRORS\nWhen you run the Red Hat AI Inference Server container image without specifying a user\nnamespace, an unrecognized model error is returned.\nExample output\nTo resolve this error, pass \n--userns=keep-id:uid=1001\n as a Podman parameter to ensure that\nthe container runs with the root user.\nSometimes when Red Hat AI Inference Server downloads the model, the download fails or gets\nstuck. To prevent the model download from hanging, first download the model using the \nhuggingface-cli\n. For example:\nWhen serving the model, pass the local model path to vLLM to prevent the model from being\ndownloaded again.\nWhen Red Hat AI Inference Server loads a model from disk, the process sometimes hangs. Large\nmodels consume memory, and if memory runs low, the system slows down as it swaps data\nbetween RAM and disk. Slow network file system speeds or a lack of available memory can\n$ export VLLM_LOGGING_LEVEL=DEBUG\npodman run --rm -it \\\n--device nvidia.com/gpu=all \\\n--security-opt=label=disable \\\n--shm-size=4GB -p 8000:8000 \\\n--env \"HUGGING_FACE_HUB_TOKEN=$HF_TOKEN\" \\\n--env \"HF_HUB_OFFLINE=0\" \\\n--env=VLLM_NO_USAGE_STATS=1 \\\n-v ./rhaiis-cache:/opt/app-root/src/.cache \\\nregistry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0 \\\n--model RedHatAI/Llama-3.2-1B-Instruct-FP8\nValueError: Unrecognized model in RedHatAI/Llama-3.2-1B-Instruct-FP8. Should have a\n \nmodel_type key in its config.json\n$ huggingface-cli download <MODEL_ID> --local-dir <DOWNLOAD_PATH>\nCHAPTER 5. TROUBLESHOOTING\n13\ntrigger excessive swapping. This can happen in clusters where file systems are shared between\ncluster nodes.\nWhere possible, store the model in a local disk to prevent slow down during model loading.\nEnsure that the system has sufficient CPU memory available.\nEnsure that your system has enough CPU capacity to handle the model.\nSometimes, Red Hat AI Inference Server fails to inspect the model. Errors are reported in the\nlog. For example:\nThe error occurs when vLLM fails to import the model file, which is usually related to missing\ndependencies or outdated binaries in the vLLM build.\nSome model architectures are not supported. Refer to the list of \nValidated models\n. For\nexample, the following errors indicate that the model you are trying to use is not supported:\nNOTE\nSome architectures such as \nDeepSeekV2VL\n require the architecture to be\nexplicitly specified using the \n--hf_overrides\n flag, for example:\nSometimes a runtime error occurs for certain hardware when you load 8-bit floating point (FP8)\nmodels. FP8 requires GPU hardware acceleration. Errors occur when you load FP8 models like \ndeepseek-r1\n or models tagged with the \nF8_E4M3\n tensor type. For example:\n#...\n  File \"vllm/model_executor/models/registry.py\", line xxx, in \\_raise_for_unsupported\n    raise ValueError(\nValueError: Model architectures [''] failed to be inspected. Please check the logs for more\n \ndetails.\nTraceback (most recent call last):\n#...\n  File \"vllm/model_executor/models/registry.py\", line xxx, in inspect_model_cls\n    for arch in architectures:\nTypeError: 'NoneType' object is not iterable\n#...\n  File \"vllm/model_executor/models/registry.py\", line xxx, in \\_raise_for_unsupported\n    raise ValueError(\nValueError: Model architectures [''] are not supported for now. Supported architectures:\n#...\n--hf_overrides '{\\\"architectures\\\": [\\\"DeepseekVLV2ForCausalLM\\\"]}\ntriton.compiler.errors.CompilationError: at 1:0:\ndef \\_per_token_group_quant_fp8(\n\\^\nValueError(\"type fp8e4nv not supported in this architecture. The supported fp8 dtypes are\n \n('fp8e4b15', 'fp8e5')\")\n[rank0]:[W502 11:12:56.323757996 ProcessGroupNCCL.cpp:1496] Warning: WARNING:\n \nRed Hat AI Inference Server 3.2 Getting started\n14\nNOTE\nReview \nGetting started\n to ensure your specific accelerator is supported.\nAccelerators that are currently supported for FP8 models include:\nNVIDIA CUDA T4, A100, L4, L40S, H100, and H200 GPUs\nAMD ROCm MI300X GPUs\nSometimes when serving a model a runtime error occurs that is related to the host system. For\nexample, you might see errors in the log like this:\nYou can work around this issue by passing the \n--shm-size=2g\n argument when starting \nvllm\n.\n5.2. MEMORY OPTIMIZATION\nIf the model is too large to run with a single GPU, you will get out-of-memory (OOM) errors. Use\nmemory optimization options such as quantization, tensor parallelism, or reduced precision to\nreduce the memory consumption. For more information, see \nConserving memory\n.\n5.3. GENERATED MODEL RESPONSE QUALITY\nIn some scenarios, the quality of the generated model responses might deteriorate after an\nupdate.\nDefault sampling parameters source have been updated in newer versions. For vLLM version\n0.8.4 and higher, the default sampling parameters come from the \ngeneration_config.json\n file\nthat is provided by the model creator. In most cases, this should lead to higher quality responses,\nbecause the model creator is likely to know which sampling parameters are best for their model.\nHowever, in some cases the defaults provided by the model creator can lead to degraded\nperformance.\nIf you experience this problem, try serving the model with the old defaults by using the \n--\ngeneration-config vllm\n server argument.\nIMPORTANT\ndestroy_process_group() was not called before program exit, which can leak resources. For\n \nmore info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function\n \noperator())\nINFO 05-07 19:15:17 [config.py:1901] Chunked prefill is enabled with\n \nmax_num_batched_tokens=2048.\nOMP: Error #179: Function Can't open SHM failed:\nOMP: System error #0: Success\nTraceback (most recent call last):\n  File \"/opt/app-root/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n..........................    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above.\nCHAPTER 5. TROUBLESHOOTING\n15\nIMPORTANT\nIf applying the \n--generation-config vllm\n server argument improves the model\noutput, continue to use the vLLM defaults and petition the model creator on\nHugging Face\n to update their default \ngeneration_config.json\n so that it\nproduces better quality generations.\n5.4. CUDA ACCELERATOR ERRORS\nYou might experience a \nself.graph.replay()\n error when running a model using CUDA\naccelerators.\nIf vLLM crashes and the error trace captures the error somewhere around the \nself.graph.replay()\n method in the \nvllm/worker/model_runner.py\n module, this is most likely a\nCUDA error that occurs inside the \nCUDAGraph\n class.\nTo identify the particular CUDA operation that causes the error, add the \n--enforce-eager\n server\nargument to the \nvllm\n command line to disable \nCUDAGraph\n optimization and isolate the\nproblematic CUDA operation.\nYou might experience accelerator and CPU communication problems that are caused by\nincorrect hardware or driver settings.\nNVIDIA Fabric Manager is required for multi-GPU systems for some types of NVIDIA GPUs. The\nnvidia-fabricmanager\n package and associated systemd service might not be installed or the\npackage might not be running.\nRun the \ndiagnostic Python script\n to check whether the NVIDIA Collective Communications\nLibrary (NCCL) and Gloo library components are communicating correctly.\nOn an NVIDIA system, check the fabric manager status by running the following command:\nOn successfully configured systems, the service should be active and running with no errors.\nRunning vLLM with tensor parallelism enabled and setting \n--tensor-parallel-size\n to be greater\nthan 1 on NVIDIA Multi-Instance GPU (MIG) hardware causes an \nAssertionError\n during the\ninitial model loading or shape checking phase. This typically occurs as one of the first errors\nwhen starting vLLM.\n5.5. NETWORKING ERRORS\nYou might experience network errors with complicated network configurations.\nTo troubleshoot network issues, search the logs for DEBUG statements where an incorrect IP\naddress is listed, for example:\nTo correct the issue, set the correct IP address with the \nVLLM_HOST_IP\n environment variable,\nfor example:\n$ systemctl status nvidia-fabricmanager\nDEBUG 06-10 21:32:17 parallel_state.py:88] world_size=8 rank=0 local_rank=0\n \ndistributed_init_method=tcp://<incorrect_ip_address>:54641 backend=nccl\n$ export VLLM_HOST_IP=<correct_ip_address>\nRed Hat AI Inference Server 3.2 Getting started\n16\nSpecify the network interface that is tied to the IP address for NCCL and Gloo:\n5.6. PYTHON MULTIPROCESSING ERRORS\nYou might experience Python multiprocessing warnings or runtime errors. This can be caused by\ncode that is not properly structured for Python multiprocessing. The following is an example\nconsole warning:\nThe following is an example Python runtime error:\nTo resolve the runtime error, update your Python code to guard the usage of \nvllm\n behind an \nif__name__ = \"__main__\":\n block, for example:\n5.7. GPU DRIVER OR DEVICE PASS-THROUGH ISSUES\nWhen you run the Red Hat AI Inference Server container image, sometimes it is unclear whether\ndevice pass-through errors are being caused by GPU drivers or tools such as the NVIDIA\nContainer Toolkit.\nCheck that the NVIDIA Container toolkit that is installed on the host machine can see the\nhost GPUs:\n$ export NCCL_SOCKET_IFNAME=<your_network_interface>\n$ export GLOO_SOCKET_IFNAME=<your_network_interface>\nWARNING 12-11 14:50:37 multiproc_worker_utils.py:281] CUDA was previously\n    initialized. We must use the `spawn` multiprocessing start method. Setting\n    VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See\n    https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing\n    for more information.\nRuntimeError:\n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ = \"__main__\":\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n        To fix this issue, refer to the \"Safe importing of main module\"\n        section in https://docs.python.org/3/library/multiprocessing.html\nif\n __name__ = \n\"__main__\"\n:\n    \nimport\n vllm\n    llm = vllm.LLM(...)\nCHAPTER 5. TROUBLESHOOTING\n17\nCheck that the NVIDIA Container toolkit that is installed on the host machine can see the\nhost GPUs:\nExample output\nEnsure that the NVIDIA accelerator configuration has been created on the host machine:\nCheck that the Red Hat AI Inference Server container can access NVIDIA GPUs on the host\nby running the following command:\nExample output\n$ nvidia-ctk cdi list\n#...\nnvidia.com/gpu=GPU-0fe9bb20-207e-90bf-71a7-677e4627d9a1\nnvidia.com/gpu=GPU-10eff114-f824-a804-e7b7-e07e3f8ebc26\nnvidia.com/gpu=GPU-39af96b4-f115-9b6d-5be9-68af3abd0e52\nnvidia.com/gpu=GPU-3a711e90-a1c5-3d32-a2cd-0abeaa3df073\nnvidia.com/gpu=GPU-6f5f6d46-3fc1-8266-5baf-582a4de11937\nnvidia.com/gpu=GPU-da30e69a-7ba3-dc81-8a8b-e9b3c30aa593\nnvidia.com/gpu=GPU-dc3c1c36-841b-bb2e-4481-381f614e6667\nnvidia.com/gpu=GPU-e85ffe36-1642-47c2-644e-76f8a0f02ba7\nnvidia.com/gpu=all\n$ sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n$ podman run --rm -it --security-opt=label=disable --device nvidia.com/gpu=all\n \nnvcr.io/nvidia/cuda:12.4.1-base-ubi9 nvidia-smi\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M.\n \n|\n|                                         |                        |               MIG M. |\n|=========================================+========================+====\n==================|\n|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:08:01.0 Off |                    0 |\n| N/A   32C    P0             64W /  400W |       1MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA A100-SXM4-80GB          Off |   00000000:08:02.0 Off |                    0 |\n| N/A   29C    P0             63W /  400W |       1MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=======================================================================\nRed Hat AI Inference Server 3.2 Getting started\n18\n==================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nCHAPTER 5. TROUBLESHOOTING\n19\n",
  "metadata": {
    "filename": "Red_Hat_AI_Inference_Server-3.2-Getting_started-en-US.pdf",
    "format": "text",
    "processed_at": "2025-07-28T23:51:36.491441",
    "pages": 23,
    "method": "PyPDF2_fallback"
  },
  "processed_at": "2025-07-28T23:51:36.491514",
  "success": true
}