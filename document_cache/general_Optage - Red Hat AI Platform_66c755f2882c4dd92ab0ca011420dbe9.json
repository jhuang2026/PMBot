{
  "file_path": "documents/general/Optage - Red Hat AI Platform.pdf",
  "product": "general",
  "product_name": "Red Hat AI General",
  "filename": "Optage - Red Hat AI Platform.pdf",
  "content": "<!-- image -->\n\n## Introduction to Red Hat AI\n\nAccelerate development and delivery of AI solutions\n\nTushar Katarki\n\nSenior Director\n\nRed Hat AI Products\n\n<!-- image -->\n\n<!-- image -->\n\n## The World Changed in November 2022\n\n## ChatGPT woke the world up to the power of generative AI\n\n<!-- image -->\n\n<!-- image -->\n\n## Generative AI is a strategic enabler across industries\n\nAI use cases that drive productivity and efficiency\n\n<!-- image -->\n\nEvery business function\n\n<!-- image -->\n\nEvery vertical industry\n\n<!-- image -->\n\n## AI is a strategic enabler across industries\n\nPredictive AI runs businesses today, Generative AI brings innovation to the enterprise\n\n## Revenue Generation\n\n## Cost Optimization\n\n## Risk Management\n\n- ▸ Chatbots\n- ▸ Campaign and Content Marketing\n- ▸ Developer assistants\n- ▸ Guided selling\n- ▸ Drive product innovation\n- ▸ Automated AI support\n- ▸ Knowledgebase Search &amp; Summarization\n- ▸ Doc summarization\n- ▸ AI-optimized logistics\n- ▸ Augmented Product R&amp;D\n- ▸ Sentiment analysis\n- ▸ Predict employee attrition\n- ▸ Contract risk assessment\n- ▸ Fraud detection\n- ▸ AI-assisted Security Operations\n\n<!-- image -->\n\n## Generative AI customer adoption challenges\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Cost\n\nGenerative AI frontier model services are cost prohibitive at scale for most enterprise customer use cases.\n\n## Complexity\n\nTuning models with private enterprise data for customer use cases is too complex for non-data scientists.\n\n## Flexibility\n\nEnterprise AI use cases span data center, cloud &amp; edge and can't be constrained to a single public cloud service.\n\n<!-- image -->\n\n<!-- image -->\n\nIncrease efficiency with fast, flexible and efficient inferencing\n\nFlexibility and consistency when scaling AI across the hybrid cloud\n\n## Accelerate the development and delivery of AI solutions\n\n## across hybrid-cloud environments\n\nSimplified and consistent experience for connecting models to data\n\nAccelerate Agentic AI delivery and stay at the forefront of innovation\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nPhysical intel\n\n<!-- image -->\n\nHardware Acceleration\n\n<!-- image -->\n\nVirtual\n\n<!-- image -->\n\nPrivate\n\nCloud\n\n<!-- image -->\n\nPublic\n\nCloud\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Fast, flexible and scalable inference\n\n<!-- image -->\n\n## The requirements of an enterprise AI production systems\n\nIdentifying the tradeoffs of inference\n\n<!-- image -->\n\nManage processing times and token output to control cost\n\nDeliver high throughput and lower latency for best performance\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Red Hat AI helps address the trade-off challenges of inference\n\nGain consistent, fast and cost-effective inference at scale with vLLM\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Select an optimal LLM\n\nRed Hat AI third party validated and compressed models ready-to-use\n\nAn efficient inference runtime\n\n<!-- image -->\n\n## Broad support for hardware\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nAMDZ\n\nintel\n\n<!-- image -->\n\n## The power of open\n\n## There has been an explosion of capability from open-source over the last 2 years\n\nApr 2025\n\n<!-- image -->\n\n<!-- image -->\n\n12\n\n## Red Hat AI the inference engine for the hybrid cloud\n\nvLLM supports the key models on the key hardware accelerators\n\n<!-- image -->\n\n## The value of vLLM\n\n## Deliver fast, flexible and scalable inference\n\n## Faster response time\n\n## Reduce hardware costs\n\nvLLM can achieve higher throughput, this translates to processing more tasks or requests within a given amount of time.\n\n## Efficient memory management\n\nvLLM organizes virtual memory, this translates to handling larger models and longer sequences more effectively within a given hardware setup.\n\nvLLM offers a more efficient use of resources, which is equivalent to fewer GPUs needed to handle the processing of LLMs.\n\n## Designed for security and scale\n\nSelf-hosting an LLM with vLLM provides you with more control over data privacy and usage, as well as an ability to handle growing demand.\n\n<!-- image -->\n\n## Red Hat AI repository on Hugging Face\n\nA collection of third-party validated and optimized large language models\n\n<!-- image -->\n\n## Validated models\n\n- ▸ Tested using realistic scenarios\n\n<!-- image -->\n\n## Optimized models\n\n<!-- image -->\n\n- ▸ Assessed for performance across a range of hardware\n- ▸ Done using GuideLLM benchmarking and LM Eval Harness\n- ▸ Compressed for speed and efficiency\n- ▸ Designed to run faster, use fewer resources, maintain accuracy\n- ▸ Done using LLM Compressor with latest algorithms\n\n<!-- image -->\n\n## Red Hat AI tooling for model optimization\n\nOptimize and validate your choice of model\n\n<!-- image -->\n\n<!-- image -->\n\n## Inference benchmarks with GuideLLM\n\nTool for evaluating LLM performance to guarantee efficient, scalable, and affordable inference serving.\n\n## Accuracy evaluation with LM-eval-harness\n\nA unified framework for evaluating the accuracy of LLMs across a variety of tasks and benchmarks.\n\n<!-- image -->\n\n## LLM Compression tools\n\nFramework for reducing the size and computational requirements of a LLMs while preserving accuracy\n\n<!-- image -->\n\n## Ex. Compressed DeepSeek-R1 models\n\nState-of-the-art, open-source quantized reasoning models built on the DeepSeek-R1-Distill\n\n- ▸ FP8 and INT8 quantized versions achieve near-perfect accuracy recovery  across all tested reasoning benchmarks and model sizes —except for the smallest INT8 1.5B model, which reaches 97%.\n- ▸ INT4 models recover 97%+ accuracy  for 7B and larger models, with the 1.5B model maintaining ~94%.\n- ▸ With vLLM 0.7.2, deliver 4X better inference performance across many common inference scenarios.\n- ▸ Reduce GPU requirements  by (e.g. 50% for INT8)\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Connecting models to data\n\n<!-- image -->\n\n## Enterprises need models aligned to their private data\n\nLLMs are trained with a range of public data, not enterprise-relevant data\n\n<!-- image -->\n\nLess than 1%  of all enterprise data is represented in foundation models\n\n## Enterprise organizations need to\n\n- 1. Start from a trusted base model\n- 2. Create a new representation of their data\n- 3. Deploy, scale, and create value with their AI\n\n<!-- image -->\n\n## The value of open source and smaller language models\n\n## Smaller models are more efficient &amp; customizable\n\n<!-- image -->\n\n- ▸ Open source AI models are catching up to proprietary models.\n- ▸ Smaller language models, like  IBM Granite, are orders of magnitude smaller than frontier models.\n- ■ Models with less than 10 billion parameters are cheaper and faster to run, and consume less energy.\n- ▸ These models can be tuned and customized with private enterprise data  for domain specific tasks.\n- ▸ Customers own their own models and can create multiple instances for different use cases and deployment environments.\n\n<!-- image -->\n\n## Granite models\n\n## A family of open, performant and trusted AI models to accelerate enterprise AI adoption\n\n## Open\n\n## Performant\n\nOpen source under the apache 2.0 license and available on watsonx.ai, Hugging Face, and other platforms.\n\n## Trusted\n\nTrained on trusted and governed data relevant to enterprise domains. Models offer IP indemnification and support.\n\nDiverse range of smaller, fit-for-purpose models that deliver performance on par with similar-sized models.\n\n## Cost-effective\n\nOffer lower cost of inference, and lower infrastructure hosting costs.\n\n<!-- image -->\n\nFoundation models for: code, language, time series, agents, safety via the Granite Guardian companion model, and even geospatial data\n\n<!-- image -->\n\nCustomize your preferred model using enterprise data to build an efficient, cost-effective solution.\n\n## Red Hat AI provides:\n\n- ✓ Validated and optimized models ready-to-use\n- ✓ Data ingestion capabilities\n- ✓ Synthetic data generation pipelines\n- ✓ Multiple alignment techniques\n\n<!-- image -->\n\n<!-- image -->\n\n## Red Hat AI provides multiple model alignment approaches\n\nBuild customized AI solutions that address domain specific business cases\n\n## Fine tuning\n\nRetrieval Augmented Generation\n\n<!-- image -->\n\n## Enhance Gen AI model generated\n\ntext by retrieving relevant information from external sources, improving accuracy and depth of model's responses.\n\nLarge-scale Alignment for chatBots\n\n<!-- image -->\n\nLeverage a taxonomy-guided synthetic data generation  process and a multi-phase tuning framework to improve model performance.\n\nFine Tuning, LoRa and QLora\n\n<!-- image -->\n\nAdjust a pre-trained model on specific tasks or data, improving its performance and accuracy for specialized applications without full retraining.\n\n<!-- image -->\n\nThe value of enterprise data can be seen in tuning smaller, targeted, optimized models to deliver state-of-the-art performance at considerably lower cost.\n\n<!-- image -->\n\n*SaaS cost per million tokes (assuming blend of 80% input, 20% output), https://artificialanalysis.ai/models/prompt-options/multiple/medium#pricing\n\n<!-- image -->\n\n## Scaling AI across the hybrid cloud\n\n<!-- image -->\n\n## Components of an AI platform\n\nSuccessful AI implementations require more than models and GPUs\n\nAccess to frontier models\n\nServing mechanisms\n\n<!-- image -->\n\nMonitoring\n\nHybrid-cloud support\n\nScalability and automation\n\nModel Lifecycle management\n\nResource optimization and management\n\n<!-- image -->\n\nRed Hat AI provides a platform for consistently building, deploying and running AI models, AI-enabled applications, and AI agents across the hybrid cloud at scale.\n\n## It provides:\n\n- ▸ An efficient inference runtime (vLLM)\n- ▸ Validated and optimized third-party models\n- ▸ InstructLab and RAG for customization\n- ▸ MLOps and LLMOps capabilities\n- ▸ Monitoring, bias detection and guardrails\n\n<!-- image -->\n\n## AI platform\n\n## Hardware Acceleration\n\n<!-- image -->\n\nVirtual\n\n<!-- image -->\n\n<!-- image -->\n\nPrivate\n\nCloud\n\nPublic\n\nCloud\n\n<!-- image -->\n\nPhysical\n\n<!-- image -->\n\nEdge\n\n<!-- image -->\n\n## Hybrid cloud deployment for AI\n\nAcross different hardware accelerators, on-prem OEM servers, and cloud environments\n\n## Cloud Environments\n\n<!-- image -->\n\n<!-- image -->\n\n## Scale and optimize your AI and application deployments\n\nExisting investments must work in support of AI\n\n<!-- image -->\n\n- ▸ Integrate to real workflows  with access to data sources, workloads and applications.\n- ▸ Think of day 2 operations  for governance, management and automation.\n- ▸ Scale AI workloads dynamically across hybrid cloud using Kubernetes, including horizontal and GPU scaling with automated resource management to meet fluctuating demands.\n\n<!-- image -->\n\n<!-- image -->\n\n## Accelerate Agentic AI innovation\n\n<!-- image -->\n\n## AI agents integrate models, functions &amp; tools\n\nGen AI Models, Predictive AI Models, Code Functions, Search &amp; more\n\n<!-- image -->\n\n<!-- image -->\n\n## The components of an AI Agent system\n\n<!-- image -->\n\n<!-- image -->\n\n## Red Hat AI provides an agile, stable foundation to accelerate the development and deployment of AI agentic workflows.\n\n- ▸ Offers built-in agent frameworks with Llama Stack, and standardized communication protocols (MCP).\n- ▸ Provides the flexibility to integrate preferred tools like LangChain and Crew AI.\n- ▸ Allows running and managing agents as microservices.\n- ▸ Simplifies production deployment by managing LLM serving and scaling.\n\n<!-- image -->\n\n<!-- image -->\n\n## A modular approach to building AI agents\n\n<!-- image -->\n\n## Hardware accelerators\n\nDeploy anywhere\n\n## Red Hat AI allows to:\n\n- ▸ Build agents using Llama Stack's native capabilities and implementations .\n- ▸ Bring compatible Llama Stack implementations to OpenShift AI.\n- ▸ Use your own agent framework k and selectively incorporate Llama Stack APIs.\n- ▸ Build with Core Primitives  and manage your own agent framework as a standard workloads.\n\n<!-- image -->\n\n<!-- image -->\n\n## Why Red Hat AI?\n\n<!-- image -->\n\n## Red Hat AI supports each stage of the AI adoption journey\n\nFrom single server deployments to highly scaled-out platform architectures\n\n<!-- image -->\n\n1010\n\n1010\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nData\n\nModel server\n\nTrusted and consistent foundation\n\n<!-- image -->\n\nSingle server GPU\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nGenerative Models\n\nPredictive Models\n\nAI-enabled workloads\n\nData connections\n\nData\n\nModel servers (1, 2 …n)\n\nContainers\n\nStorage\n\nTrusted, Consistent and Comprehensive foundation\n\n## Cluster\n\nHardware Acceleration\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Increasing flexibility and choice with an open source approach\n\nRed Hat prioritizes  investments on open source AI and building a certified AI partner ecosystem\n\n<!-- image -->\n\nFlexibility\n\n<!-- image -->\n\nChoice\n\nAccess to cutting-edge open source innovations to keep up with a fast moving market.\n\nAccess to an open ecosystem of communities, technology providers, ISVs and customers.\n\n<!-- image -->\n\nReduce the complexity of switching and adapting to new technologies.\n\n<!-- image -->\n\n<!-- image -->\n\n## Increased efficiency\n\nReduce development and deployment costs with access to optimized open source models\n\n## The value of Red Hat AI\n\n<!-- image -->\n\n## Simplified experience\n\nEnable developers, data scientists and domain experts to tailor models more efficiently\n\n<!-- image -->\n\n## Flexibility to deploy anywhere\n\nMitigate risks, reduce costs, and scale your AI deployments across the hybrid cloud\n\n<!-- image -->\n\n## U.S. Department of Veterans Affairs\n\nSuicide has no single cause, and no single strategy can end this complex problem. That's why Mission Daybreak is fostering solutions across a broad spectrum of focus areas.\n\nA diversity of solutions will only be possible if a diversity of solvers answer the call to collaborate and share their expertise.\n\n## Red Hat, Team Guidehouse named winner in Mission Daybreak challenge to reduce veteran suicides\n\n## Challenge\n\nDevelop new data-driven means of identifying veterans at risk for suicide\n\n## Solution\n\nRed Hat teamed with global consulting services provider Guidehouse and Philip Held, Ph.D. of Rush University Medical Center, to develop a new data-driven means of identifying veterans at risk for suicide running on Red Hat  technologies.\n\n## Results\n\n- ● Allows providers to more easily identify and help specific veterans in need,  using artificial intelligence and machine learning to sift through vast volumes of data.\n- ● Offers an API-first approach that streamlines integration into existing systems, providing ready access to medical histories that are key to identify veterans at risk in support of timely interventions.\n- ● Uses a managed cloud service for data scientists and developers to rapidly develop, train and test machine learning models  in the public cloud before deploying to production\n\n<!-- image -->\n\n<!-- image -->\n\n\"Red Hat's work with AGESIC exemplifies our dedication to improving the user experience for both our and their customers.\"\n\nSteven Huels Vice President and General Manager – AI Business Unit, Red Hat\n\n<!-- image -->\n\n## Presentation abstract\n\nAGESIC, Uruguay's Agency for Electronic Government and Information and Knowledge Society, is responsible for e-government strategy and implementation. With Red Hat®, it led Uruguay's AI strategy and provided a more consistent, hybrid AI/ML platform to build and host models while delivering innovative applications.\n\n## Presentation summary\n\n- ● With the proliferation of AI, AGESIC knew that infusing it into its operations would be key to meeting Uruguay's evolving needs.\n- ● AGESIC optimized its AI infrastructure with Red Hat OpenShift®, which brought a containerized approach to workload management and automation of key processes w while also bringing development, operations, and systems security functions together on a centralized platform.\n- ● AGESIC evolved its offerings to include Platform as a Service (PaaS), enabling other government agencies to develop, run, and manage applications without the build and maintenance of complex infrastructure.\n- ● AGESIC has begun automating the creation and development process of its AI models and managing model lifecycles, which has enabled standardization of AI usage across all Uruguayan governmental agencies\n\n## Products and services\n\nRed Hat OpenShift\n\n<!-- image -->\n\n<!-- image -->\n\n\"As an invaluable AI-driven solution, Red Hat OpenShift AI provides a streamlined environment that enables our data scientists to build and deploy more robust and secure models.\"\n\nOkan Çetinkaya CDO – CAO\n\nDenizBank\n\n## DenizBank transforms AI operations and empowers innovation\n\n## Challenge\n\nIntertech - IT subsidiary of DenizBank - wanted to build a comprehensive, standardized, holistic solution for data scientists that would improve time to market while delivering AI/ML process cost efficiencies across multiple business lines, including risk management, marketing and customer relations.\n\n## Solution\n\nRed Hat Consulting helped the team design and architect the Red Hat OpenShift AI solution on premise - providing self-service capabilities and capacity to scale model serving while improving operational efficiency.\n\n## Results\n\n- ● Provided more than 120 data scientists, from different lines of business, with greater autonomy and more consistent standards\n- ● Accelerated time-to-market while ensuring more robust and secure models\n- ● Optimized GPU usage with slicing\n\n<!-- image -->\n\nRed Hat Skills Assessment\n\nPrototype\n\n## Services offerings for Red Hat AI\n\nLearn how to maximize your technology investments\n\n## Training and Certification: Developing and Deploying AI/ML Applications on Red Hat OpenShift AI with Exam (AI268)\n\nDeployment\n\nScale\n\n## AI Incubator\n\nRapid prototyping of use cases in a controlled environment\n\n- ● Rapidly prototype AI applications and services\n- ● Develop RAG+RAFT based patterns for model tuning and training\n- ● Prototype AI Assistants &amp; chatbots\n- ● Develop evaluations for model accuracy and speed\n- ● Prototype data ingestion pipelines\n\n<!-- image -->\n\n## AI Platform Foundation\n\nAutomated deploy of Red Hat AI Platform while advancing your AI practices\n\n- ● Upskill customer's ML Platform team and data scientists\n- ● Help customers adopt new AI capabilities\n- ● Layout future roadmap of skills and capabilities\n- ● Increase teams core MLOps competency\n\n<!-- image -->\n\n<!-- image -->\n\nOperational guidance &amp; advisory services from TAM Services for Red Hat AI Platform (yearly subscription)\n\n## MLOps Foundation\n\nRoll out automated MLOps pipelines and practices throughout your organization\n\n- ● Establish self-service of MLOps platforms\n- ● Automate and template ML pipelines\n- ● Establish patterns and best practices for managing production ready solutions\n\n<!-- image -->\n\n<!-- image -->\n\n## AI Incubator  powered by Red Hat Composer AI\n\nGenerative AI Use Case Development Made Easy\n\nThe AI Incubator is a consulting approach designed to accelerate innovation by providing a unified, scalable platform designed to rapidly transform new ideas into impactful AI solutions. Streamline the entire lifecycle of AI development, from concept to deployment, while benefiting from a robust architecture that ensures seamless integration and top-tier security. Our platform empowers your enterprise to harness cutting-edge AI technologies quickly and efficiently, all while maintaining complete control over your data and infrastructure.\n\n- ● Centralized AI Platform:  Accelerate AI use case development with an intuitive, secure, and scalable environment\n- ● Tailored for All Roles:  Empower executives, developers, and infrastructure managers with tools designed for innovation, efficiency, and control\n- ● Seamless Integration:  Works with open source models and manages proprietary data securely across various databases\n- ● Automate &amp; Innovate:  Streamline workflows, automate routine tasks, and focus on what matters most—driving your organization forward\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Next best steps you can take\n\nLearn more and get hands-on experience\n\n## TRY RED HAT ENTERPRISE LINUX AI\n\nA single, 60-day, self-supported subscription to Red Hat® Enterprise Linux® AI\n\n## TRY RED HAT OPENSHIFT AI\n\nA single, 60-day, self-supported subscription to Red Hat® OpenShift® AI (Self-Managed)\n\n<!-- image -->\n\n<!-- image -->\n\n## PROOF OF CONCEPT (POC) DESIGNED FOR YOU\n\nLet us bring your vision to life: Request your personalized POC today!\n\n<!-- image -->\n\n<!-- image -->\n\n## Thank you\n\nRed Hat is the world's leading provider of enterprise open source software solutions. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nlinkedin.com/company/red-hat youtube.com/user/RedHatVideos\n\nfacebook.com/redhatinc twitter.com/RedHat\n\n<!-- image -->\n\n## Red Hat AI Announcements\n\n<!-- image -->\n\n<!-- image -->\n\n## vLLM Inference Server in Red Hat AI\n\nvLLM connects model creators to accelerated hardware providers\n\n## Model creators\n\n## Hardware Vendors\n\n## Contribution Trajectory\n\n<!-- image -->\n\n<!-- image -->\n\n| Features for new HW   |\n|-----------------------|\n| Choice for MI300X     |\n| Gaudi enablement      |\n| TPU enablement        |\n| Neuron enablement     |\n| Spyre enablement      |\n\n<!-- image -->\n\n<!-- image -->\n\n## Red Hat AI platform\n\nGenerative AI, Predictive AI &amp; MLOps capabilities for building flexible, trusted AI solutions at scale\n\n<!-- image -->\n\n<!-- image -->",
  "metadata": {
    "filename": "Optage - Red Hat AI Platform.pdf",
    "format": "markdown",
    "processed_at": "2025-07-28T23:37:31.296912",
    "pages": 0,
    "tables": 0,
    "formulas": 0,
    "endpoint_used": "https://docling-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1alpha/convert/source",
    "processing_time": 50.04666789388284,
    "method": "Docling_API"
  },
  "processed_at": "2025-07-28T23:37:31.299812",
  "success": true
}