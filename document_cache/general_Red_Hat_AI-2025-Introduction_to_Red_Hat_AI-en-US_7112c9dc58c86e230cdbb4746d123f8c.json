{
  "file_path": "documents/general/Red_Hat_AI-2025-Introduction_to_Red_Hat_AI-en-US.pdf",
  "product": "general",
  "product_name": "Red Hat AI General",
  "filename": "Red_Hat_AI-2025-Introduction_to_Red_Hat_AI-en-US.pdf",
  "content": "<!-- image -->\n\n## Red Hat AI 2025 Introduction to Red Hat AI\n\nRed Hat AI is a portfolio of products and services that accelerates the development and deployment of AI solutions across hybrid cloud environments\n\n## Red Hat AI 2025 Introduction to Red Hat AI\n\nRed Hat AI is a portfolio of products and services that accelerates the development and deployment of AI solutions across hybrid cloud environments\n\n## Legal Notice\n\nCopyright © 2025 Red Hat , Inc .\n\nThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution – Share Alike 3 . 0 Unported license ( \" CC -BY -SA \" ) . An explanation of CC-BY-SA is available at http:////creativecommons . org/licenses/s/by-sa/3 . 0/\n\n- . In accordance with CC -BY -SA , if you distribute this document or an adaptation of it , you must provide the URL for the original version .\n\nRed Hat , as the licensor of this document , waives the right to enforce , and agrees not to assert , Section 4d of CC -BY -SA to the fullest extent permitted by applicable law .\n\nRed Hat , Red Hat Enterprise Linux , the Shadowman logo , the Red Hat logo , JBoss , OpenShift , Fedora , the Infinity logo , and RHCE are trademarks of Red Hat , Inc . , registered in the United States and other countries .\n\nLinux ® is the registered trademark of Linus Torvalds in the United States and other countries .\n\nJava ® is a registered trademark of Oracle and/or its affiliates .\n\nXFS ® is a trademark of Silicon Graphics International Corp . or its subsidiaries in the United States and/or other countries .\n\nMySQL ® is a registered trademark of MySQL AB in the United States , the European Union and other countries .\n\nNode . js ® is an official trademark of Joyent . Red Hat is not formally related to or endorsed by the official Joyent Node . js open source or commercial project .\n\nThe OpenStack ® Word Mark and OpenStack logo are either registered trademarks/s/service marks or trademarks/s/service marks of the OpenStack Foundation , in the United States and other countries and are used with the OpenStack Foundation ' s permission . We are not affiliated with , endorsed or sponsored by the OpenStack Foundation , or the OpenStack community .\n\nAll other trademarks are the property of their respective owners .\n\n## Abstract\n\nWith Red Hat AI , organizations have the flexibility and consistency to deploy and manage both predictive and generative AI models wherever it makes the most sense for their AI workload strategy .\n\n## Table of Contents\n\n| . \r C . C . H . A . A . P . T . T . E . R  . R  . 1 .  . N . N . T . R . R . O . D . D . U . UC . C . I . N . G  . G  .  . R . E . D  . D  . H . H . A . AT  . T  . A . A .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . \r C. H. . A. P. . T. E. . R . 1 .  I. . N. T. . R. O. . D. . U. . C. I. N. . G . R. E. . D . . H. . A. . T . . AI   |   .  . 3 . 3 |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|\n| 1 . 1 .  UNDERSTANDING RED HAT ENTERPRISE LINUX AI                                                                                                                                                                                                                                                                                                                                       |            3 |\n| 1 . 1 . 1 .  Key benefits of RHEL AI                                                                                                                                                                                                                                                                                                                                                     |            4 |\n| 1 . 1 . 1 . 1 .  Installation and deployment                                                                                                                                                                                                                                                                                                                                             |            4 |\n| 1 . 1 . 1 . 2 .  Model customization                                                                                                                                                                                                                                                                                                                                                     |            4 |\n| 1 . 2 .  UNDERSTANDING RED HAT OPENSHIFT AI                                                                                                                                                                                                                                                                                                                                              |            4 |\n| 1 . 2 . 1 .  Key benefits of OpenShift AI                                                                                                                                                                                                                                                                                                                                                |            4 |\n| 1 . 2 . 2 .  Features for data scientists ,  developers ,  and MLOps engineers                                                                                                                                                                                                                                                                                                           |            5 |\n| 1 . 2 . 3 .  Features for IT operations administrators                                                                                                                                                                                                                                                                                                                                   |            5 |\n| 1 . 3 .  UNDERSTANDING RED HAT AI INFERENCE SERVER                                                                                                                                                                                                                                                                                                                                       |            5 |\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n## CHAPTER 1 . INTRODUCING RED HAT AI\n\nRed Hat AI is a product and services portfolio that accelerates time to market and reduces the operational cost of delivering artificial intelligence (AI) solutions across hybrid cloud environments . With Red Hat AI , you can efficiently tune small , fit -for -purpose models by using enterprise-relevant data and to flexibly deploy models wherever your data is stored .\n\nRed Hat AI helps you manage and monitor the lifecycle of both predictive and generative AI (gen AI) models at scale , from single-server deployments to highly distributed platforms . The portfolio is powered by open source technologies and a partner ecosystem that focuses on performance , stability , and GPU support across various infrastructures .\n\nWith Red Hat AI , you can deploy and manage both predictive and gen AI models for your AI workload strategy . The portfolio supports each stage of the AI adoption journey , from initial single-server deployments to highly scaled-out distributed platform architectures . It also provides support for multiple hardware accelerators , original equipment manufacturers (OEMs) , and cloud providers , to deliver a stable , optimized , and high performance platform across various infrastructures .\n\nAccess to the latest innovations is complemented by Red Hat ' s AI partner ecosystem , which offers tested , supported , and certified partner products and services that work with Red Hat technologies and help you solve your business and technical challenges .\n\nRed Hat AI includes:\n\n## Red Hat Enterprise Linux AI\n\nA foundation model platform for large language model (LLM) development , testing , and deployment with optimized inference capabilities .\n\nRed Hat Enterprise Linux AI can support you at the beginning of your AI journey if you haven ' t defined your business use cases yet . The AI platform is built to develop , test , and run generative AI (gen AI) foundation models .\n\n## Red Hat OpenShift AI\n\nAn integrated MLOps platform that helps you manage your artificial intelligence and machine learning (AI/ML) lifecycle across hybrid cloud and edge environments , which helps you bring models from experimentation to production faster .\n\nRed Hat OpenShift AI can support you if you are ready to scale your AI applications . This AI platform can help manage the lifecycle of both predictive and generative AI models across hybrid cloud environments .\n\n## Red Hat AI Inference Server\n\nA container image that optimizes serving and inferencing with LLMs . Using AI Inference Server , you can serve and inference models in a way that boosts performance while reducing costs .\n\n## 1 . 1 . UNDERSTANDING RED HAT ENTERPRISE LINUX AI\n\nRed Hat Enterprise Linux AI (RHEL AI) empowers you to customize and contribute directly to large language models (LLMs) . RHEL AI is built from the InstructLab project , which uses a fine -tuning approach called LAB (Large-Scale Alignment for Chatbots) . The LAB method uses synthetic data generation (SDG) with a multi-phase training framework to produce high-quality , fine -tuned LLMs .\n\nYou can install RHEL AI as a bootable Red Hat Enterprise Linux (RHEL) container image . Each image is configured for specific hardware accelerators , including NVIDIA , AMD , and Intel , and contains various inference -serving and fine-tuning tools .\n\nYou can use your own data to create seed files , generate synthetic data , and train a Granite starter model that you can interact with and deploy .\n\n## 1 . 1 . 1 . Key benefits of RHEL AI\n\n## 1 . 1 . 1 . 1 . Installation and deployment\n\n- RHEL AI is installed using the RHEL bootable containerized operating system . The RHEL AI image contains various open source fine-tuning tools so you can customize the Granite starter models provided by Red Hat .\n- RHEL AI provides images for you to deploy on bare metal , Amazon Web Services (AWS) , Azure , IBM Cloud , and Google Cloud Platform (GCP) .\n- You can purchase RHEL AI from the AWS and Azure marketplaces and deploy it on any of their GPU -enabled instances .\n- You can locally download , deploy , and chat with various models provided by Red Hat and IBM .\n\n## 1 . 1 . 1 . 2 . Model customization\n\n- You can use the Synthetic Data Generation (SDG) process , where teacher LLMs use human -generated data to generate a large quantity of artificial data that you can use to train other LLMs .\n- You can use multi -phase training , a fine -tuning framework where a model is trained on a dataset and evaluated in separate phases , called checkpoints . The final phase of training provides the most efficient , fully fine-tuned model .\n- You can use various model evaluation benchmarks , including MMLU , MT \\_ BENCH , and DK \\_ BENCH .\n\n## 1 . 2 . UNDERSTANDING RED HAT OPENSHIFT AI\n\nRed Hat OpenShift AI is a comprehensive MLOps platform designed to streamline artificial intelligence and machine learning (AI/ML) development and operations across hybrid cloud environments and the edge . It fosters collaboration between data scientists and developers while ensuring IT oversight , which empowers organizations to efficiently build , train , fine -tune , and deploy predictive and generative AI models .\n\nOffered as a self -managed or cloud service , OpenShift AI builds on the robust foundation of Red Hat OpenShift , providing a trusted platform for securely deploying AI-enabled applications and ML models at scale — across public clouds , on -premises , and edge environments .\n\nBy leveraging a broad technology ecosystem , Red Hat OpenShift AI accelerates AI/ML innovation , ensures operational consistency , enhances hybrid cloud flexibility , and upholds transparency , choice , and responsible AI practices .\n\n## 1 . 2 . 1 . Key benefits of OpenShift AI\n\n- Simplified AI adoption: Reduces the complexities of building and delivering AI models and applications that are accurate , reliable , and secure .\n- Enterprise-ready open source tools: Provides a fully supported , secure enterprise version of open -source AI tools , ensuring seamless integration and interoperability .\n\n- Accelerated innovation: Gives organizations access to the latest AI technologies , helping them stay competitive in a rapidly evolving market .\n- Extensive partner ecosystem: Enables organizations to select best-of-breed technologies from a certified AI ecosystem , increasing flexibility and choice .\n\n## 1 . 2 . 2 . Features for data scientists , developers , and MLOps engineers\n\n- Integrated development environments (IDEs): Provides access to IDEs like JupyterLab , with pre -configured libraries like TensorFlow , PyTorch , and Scikit -learn .\n- Data science pipelines: Supports end-to-end ML workflows by using containerized pipeline orchestration .\n- Accelerated computing: Integrated support for GPUs and Intel Gaudi AI accelerators to speed up model training and inference .\n- Model deployment and serving: Deploy models in a variety of environments and integrate them into applications by using APIs .\n\n## 1 . 2 . 3 . Features for IT operations administrators\n\n- Seamless OpenShift integration: Leverages OpenShift identity providers and resource allocation tools for secure and efficient user management .\n- Accelerator management: Enables efficient resource scheduling for GPU and AI accelerator usage .\n- Flexible deployment: Available as a self-managed solution or as a managed service in Red Hat OpenShift Dedicated and Red Hat OpenShift Service on AWS (ROSA) .\n- Scalability and security: y: Provides enterprise-grade security features and governance controls for AI workloads .\n\n## 1 . 3 . UNDERSTANDING RED HAT AI INFERENCE SERVER\n\nRed Hat AI Inference Server provides advanced inferencing features with enterprise-grade stability and security building on the open source vLLM project .\n\nAI Inference Server uses continuous batching and tensor parallelism to provide reduced latency and higher throughput . Continuous batching processes model requests as they arrive instead of waiting for a full batch to be accumulated . Tensor parallelism distributes LLM workloads across multiple GPUs .\n\nTo reduce the cost of inferencing models , AI Inference Server uses paged attention . LLMs use a mechanism called attention to understand conversations with users . Normally , attention uses a significant amount of memory , much of which is wasted . Paged attention addresses this memory wastage by provisioning memory for LLMs similar to the way that virtual memory works for operating systems . This approach consumes less memory , which lowers costs .\n\nRed Hat AI Inference Server has the following features:\n\n- Inference runtime for the hybrid cloud: d: Run your choice of models across accelerators , Kubernetes , and Linux environments .\n- LLM Compressor: Compress models to optimize accelerator and compute usage . Reduce costs while maintaining high model accuracy .\n\n- Optimized model repository: y: Gain access to a collection of optimized models ready for inference deployment , with support for both NVIDIA and AMD accelerators .\n- Certified for use with Red Hat products: s: Integrate with RHEL AI and OpenShift AI .",
  "metadata": {
    "filename": "Red_Hat_AI-2025-Introduction_to_Red_Hat_AI-en-US.pdf",
    "format": "markdown",
    "processed_at": "2025-07-28T23:37:52.994228",
    "pages": 0,
    "tables": 0,
    "formulas": 0,
    "endpoint_used": "https://docling-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1alpha/convert/source",
    "processing_time": 21.55624175304547,
    "method": "Docling_API"
  },
  "processed_at": "2025-07-28T23:37:52.996364",
  "success": true
}