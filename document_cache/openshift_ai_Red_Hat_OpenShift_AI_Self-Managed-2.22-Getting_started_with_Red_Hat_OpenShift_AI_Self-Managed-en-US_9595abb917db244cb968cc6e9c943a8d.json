{
  "file_path": "documents/openshift_ai/Red_Hat_OpenShift_AI_Self-Managed-2.22-Getting_started_with_Red_Hat_OpenShift_AI_Self-Managed-en-US.pdf",
  "product": "openshift_ai",
  "product_name": "Red Hat OpenShift AI",
  "filename": "Red_Hat_OpenShift_AI_Self-Managed-2.22-Getting_started_with_Red_Hat_OpenShift_AI_Self-Managed-en-US.pdf",
  "content": "Red Hat OpenShift AI Self-Managed\n2.22\nGetting started with Red Hat OpenShift AI\nSelf-Managed\nLearn how to work in an OpenShift AI environment\nLast Updated: 2025-07-23\n\nRed Hat OpenShift AI Self-Managed\n \n2.22\n \nGetting started with Red Hat\nOpenShift AI Self-Managed\nLearn how to work in an OpenShift AI environment\nLegal Notice\nCopyright \n©\n 2025 Red Hat, Inc.\nThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons\nAttribution–Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is\navailable at\nhttp://creativecommons.org/licenses/by-sa/3.0/\n. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must\nprovide the URL for the original version.\nRed Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert,\nSection 4d of CC-BY-SA to the fullest extent permitted by applicable law.\nRed Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift,\nFedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States\nand other countries.\nLinux ®\n is the registered trademark of Linus Torvalds in the United States and other countries.\nJava ®\n is a registered trademark of Oracle and/or its affiliates.\nXFS ®\n is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States\nand/or other countries.\nMySQL ®\n is a registered trademark of MySQL AB in the United States, the European Union and\nother countries.\nNode.js ®\n is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the\nofficial Joyent Node.js open source or commercial project.\nThe \nOpenStack ®\n Word Mark and OpenStack logo are either registered trademarks/service marks\nor trademarks/service marks of the OpenStack Foundation, in the United States and other\ncountries and are used with the OpenStack Foundation's permission. We are not affiliated with,\nendorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\nAll other trademarks are the property of their respective owners.\nAbstract\nLearn how to work in an OpenShift AI environment.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nTable of Contents\nCHAPTER 1. OVERVIEW\n1.1. DATA SCIENCE WORKFLOW\n1.2. ABOUT THIS GUIDE\n1.3. GLOSSARY OF COMMON TERMS\nCHAPTER 2. LOGGING IN TO OPENSHIFT AI\n2.1. VIEWING INSTALLED OPENSHIFT AI COMPONENTS\nCHAPTER 3. CREATING A DATA SCIENCE PROJECT\nCHAPTER 4. CREATING A WORKBENCH AND SELECTING AN IDE\n4.1. ABOUT WORKBENCH IMAGES\n4.2. BUILDING THE RSTUDIO SERVER WORKBENCH IMAGES\n4.3. CREATING A WORKBENCH\nCHAPTER 5. NEXT STEPS\n5.1. ADDITIONAL RESOURCES\n3\n3\n4\n4\n8\n8\n10\n12\n12\n15\n18\n23\n24\nTable of Contents\n1\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n2\nCHAPTER 1. OVERVIEW\nRed Hat OpenShift AI is an artificial intelligence (AI) platform that provides tools to rapidly train, serve,\nand monitor machine learning (ML) models onsite, in the public cloud, or at the edge.\nOpenShift AI provides a powerful AI/ML platform for building AI-enabled applications. Data scientists\nand MLOps engineers can collaborate to move from experiment to production in a consistent\nenvironment quickly.\nYou can deploy OpenShift AI on any supported version of OpenShift, whether on-premise, in the cloud,\nor in disconnected environments. For details on supported versions, see \nRed Hat OpenShift AI:\nSupported Configurations\n.\n1.1. DATA SCIENCE WORKFLOW\nFor the purpose of getting you started with OpenShift AI, the following figure illustrates a simplified\ndata science workflow. The real world process of developing ML models is an iterative one.\nFigure 1.1. Simplified data science workflow\nThe simplified data science workflow for predictive AI use cases includes the following tasks:\nDefining your business problem and setting goals to solve it.\nGathering, cleaning, and preparing data. Data often has to be federated from a range of\nsources, and exploring and understanding data plays a key role in the success of a data science\nproject.\nEvaluating and selecting ML models for your business use case.\nTrain models for your business use case by tuning model parameters based on your set of\ntraining data. In practice, data scientists train a range of models, and compare performance\nwhile considering tradeoffs such as time and memory constraints.\nIntegrate models into an application, including deployment and testing. After model training, the\nnext step of the workflow is production. Data scientists are often responsible for putting the\nmodel in production and making it accessible so that a developer can integrate the model into\nan application.\nMonitor and manage deployed models. Depending on the organization, data scientists, data\nengineers, or ML engineers must monitor the performance of models in production, tracking\nprediction and performance metrics.\nRefine and retrain models. Data scientists can evaluate model performance results and refine\nCHAPTER 1. OVERVIEW\n3\nRefine and retrain models. Data scientists can evaluate model performance results and refine\nmodels to improve outcome by excluding or including features, changing the training data, and\nmodifying other configuration parameters.\n1.2. ABOUT THIS GUIDE\nThis guide assumes you are familiar with data science and ML Ops concepts. It describes the following\ntasks to get you started with using OpenShift AI:\nLog in to the OpenShift AI dashboard\nCreate a data science project\nIf you have data stored in Object Storage, configure a connection to more easily access it\nCreate a workbench and choose an IDE, such as JupyterLab or code-server, for your data\nscientist development work\nLearn where to get information about the next steps:\nDeveloping and training a model\nAutomating the workflow with pipelines\nImplementing distributed workloads\nTesting your model\nDeploying your model\nMonitoring and managing your model\nSee also \nOpenShift AI tutorial: Fraud detection example\n. It provides step-by-step guidance for using\nOpenShift AI to develop and train an example model in JupyterLab, deploy the model, and refine the\nmodel by using automated pipelines.\n1.3. GLOSSARY OF COMMON TERMS\nThis glossary defines common terms for Red Hat OpenShift AI.\naccelerator\nIn high-performance computing, a specialized circuit that is used to take some of the computational\nload from the CPU, increasing the efficiency of the system. For example, in deep learning, GPU-\naccelerated computing is often employed to offload part of the compute workload to a GPU while\nthe main application runs off the CPU.\nartificial intelligence (AI)\nThe capability to acquire, process, create and apply knowledge in the form of a model to make\npredictions, recommendations or decisions.\nbias detection\nThe process of calculating fairness metrics to detect when AI models are delivering unfair outcomes\nbased on certain attributes.\ncustom resource (CR)\nA resource implemented through the Kubernetes CustomResourceDefinition API. A custom resource\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n4\nA resource implemented through the Kubernetes CustomResourceDefinition API. A custom resource\nis distinct from the built-in Kubernetes resources, such as the pod and service resources. Every CR is\npart of an API group.\ncustom resource definition (CRD)\nIn Red Hat OpenShift, a custom resource definition (CRD) defines a new, unique object \nKind\n in the\ncluster and lets the Kubernetes API server handle its entire lifecycle.\nconnections\nA configuration that stores the parameters required to connect to an S3-compatible object storage,\ndatabase or OCI-compliant container registry from a data science project.\nconnection type\nThe type of external source to connect to from a data science project, such as an OCI-compliant\ncontainer registry, S3-compatible object storage, or Uniform Resource Identifiers (URIs).\ndata science pipelines\nA workflow engine that is used by data scientists and AI engineers to automate pipelines, such as\nmodel training and evaluation pipelines. Data science pipelines also includes experiment tracking\ncapabilities, artifact storage, and versioning.\ndata science project\nAn OpenShift project for organizing data science work. Each project is scoped to its own Kubernetes\nnamespace.\ndisconnected environment\nAn environment on a restricted network that does not have an active connection to the internet.\ndistributed workloads\nData science workloads that are run simultaneously across multiple nodes in an OpenShift cluster.\nfine-tuning\nThe process of adapting a pre-trained model to perform a specific task by conducting additional\ntraining. Fine tuning may involve (1) updating the model’s existing parameters, known as full fine\ntuning, or (2) updating a subset of the model’s existing parameters or adding new parameters to the\nmodel and training them while freezing the model’s existing parameters, known as parameter-\nefficient fine tuning.\ngraphics processing unit (GPU)\nA specialized processor designed to rapidly manipulate and alter memory to accelerate the creation\nof images in a frame buffer intended for output to a display. GPUs are heavily utilized in machine\nlearning due to their parallel processing capabilities.\ninference\nThe process of using a trained AI model to generate predictions or conclusions based on the input\ndata provided to the model.\ninference server\nA server that performs inference. Inference servers feed the input requests through a machine\nlearning model and return an output.\nlarge language model (LLM)\nA language model with a large number of parameters, trained on a large quantity of text.\nmachine learning (ML)\nA branch of artificial intelligence (AI) and computer science that focuses on the use of data and\nalgorithms to imitate the way that humans learn, gradually improving the accuracy of AI models.\nmodel\nIn a machine learning context, a set of functions and algorithms that have been trained and tested on\nCHAPTER 1. OVERVIEW\n5\nIn a machine learning context, a set of functions and algorithms that have been trained and tested on\na data set to provide predictions or decisions.\nmodel registry\nA central repository containing metadata related to machine learning models from inception to\ndeployment. The metadata ranges from high-level information such as the deployment environment\nand project origins, to intricate details like training hyperparameters, performance metrics, and\ndeployment events.\nmodel server\nA container that hosts a machine learning model, exposes an API to handle incoming requests,\nperforms inference, and returns model predictions.\nmodel-serving runtime\nA component or framework that helps create model servers for deploying machine learning models\nand build APIs optimized for inference.\nMLOps\nThe practice for collaboration between data scientists and operations professionals to help manage\nthe production machine learning (or deep learning) lifecycle. MLOps looks to increase automation\nand improve the quality of production ML while also focusing on business and regulatory\nrequirements. It involves model development, training, validation, deployment, monitoring, and\nmanagement and uses methods like CI/CD.\nnotebook interface\nAn interactive document that contains executable code, descriptive text for that code, and the\nresults of any code that is run.\nobject storage\nA method of storing data, typically used in the cloud, in which data is stored as discrete units, or\nobjects, in a storage pool or repository that does not use a file hierarchy but that stores all objects at\nthe same level.\nOpenShift Container Platform cluster\nA group of physical machines that contains the controllers, pods, services, and configurations\nrequired to build and run containerized applications.\npersistent storage\nA persistent volume that retains files, models or other artifacts across components such as model\ndeployments, data science pipelines and workbenches.\npersistent volume claim (PVC)\nA persistent volume claim (PVC) is a request for storage in the cluster by a user.\nquantization\nA method of compressing foundation model weights to speed up inferencing and reduce memory\nneeds.\nserving\nThe process of hosting a trained machine learning model as a network-accessible service. Real-world\napplications can send inference requests to the service by using a REST or gRPC API and receive\npredictions.\nServingRuntime\nA custom resource definition (CRD) that defines the templates for pods that can serve one or more\nparticular model formats. Each ServingRuntime CRD defines key information such as the container\nimage of the runtime and a list of the model formats that the runtime supports. Other configuration\nsettings for the runtime can be conveyed through environment variables in the container\nspecification. It also dynamically loads and unloads models from disk into memory on demand and\nexposes a gRPC service endpoint to serve inferencing requests for loaded models.\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n6\nvLLM\nA high-throughput and efficient inference engine for running large-language models that integrates\nwith popular models and frameworks.\nworkbench\nAn isolated environment for development and experimentation with ML models. Workbenches\ntypically contain integrated development environments (IDEs), such as JupyterLab, RStudio, and\nVisual Studio Code.\nworkbench image\nAn image that includes preinstalled tools and libraries that you need for model development.\nIncludes an IDE for developing your machine learning (ML) models.\nYAML\nA human-readable data-serialization language. It is commonly used for configuration files and in\napplications where data is being stored or transmitted.\nCHAPTER 1. OVERVIEW\n7\nCHAPTER 2. LOGGING IN TO OPENSHIFT AI\nAfter you install OpenShift AI, log in to the OpenShift AI dashboard so that you can set up your\ndevelopment and deployment environment.\nPrerequisites\nYou know the OpenShift AI identity provider and your login credentials.\nIf you are a data scientist, data engineer, or ML engineer, your administrator must provide\nyou with the OpenShift AI instance URL, for example:\nhttps://rhoai-dashboard-redhat-oai-\napplications.apps.example.abc1.p1.openshiftapps.com/\nYou have the latest version of one of the following supported browsers:\nGoogle Chrome\nMozilla Firefox\nSafari\nProcedure\n1\n. \nBrowse to the OpenShift AI instance URL and click \nLog in with OpenShift\n.\nIf you have access to OpenShift, you can browse to the OpenShift web console and click\nthe \nApplication Launcher\n ( \n \n) \n→\n \nRed Hat OpenShift AI\n.\n2\n. \nClick the name of your identity provider, for example, \nGitHub\n,\nGoogle\n, or your company’s single\nsign-on method.\n3\n. \nEnter your credentials and click \nLog in\n (or equivalent for your identity provider).\nVerification\nThe OpenShift AI dashboard opens on the \nHome\n page.\n2.1. VIEWING INSTALLED OPENSHIFT AI COMPONENTS\nIn the Red Hat OpenShift AI dashboard, you can view a list of the installed OpenShift AI components,\ntheir corresponding source (upstream) components, and the versions of the installed components.\nPrerequisites\nOpenShift AI is installed in your OpenShift cluster.\nProcedure\n1\n. \nLog in to the OpenShift AI dashboard.\n2\n. \nIn the top navigation bar, click the help icon ( \n \n) and then select \nAbout\n.\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n8\nVerification\nThe \nAbout\n page shows a list of the installed OpenShift AI components along with their corresponding\nupstream components and upstream component versions.\nAdditional resources\nInstalling and managing Red Hat OpenShift AI components\nCHAPTER 2. LOGGING IN TO OPENSHIFT AI\n9\nCHAPTER 3. CREATING A DATA SCIENCE PROJECT\nTo implement a data science workflow, you must create a project. In OpenShift, a project is a Kubernetes\nnamespace with additional annotations, and is the main way that you can manage user access to\nresources. A project organizes your data science work in one place and also allows you to collaborate\nwith other developers and data scientists in your organization.\nWithin a project, you can add the following functionality:\nConnections so that you can access data without having to hardcode information like endpoints\nor credentials.\nWorkbenches for working with and processing data, and for developing models.\nDeployed models so that you can test them and then integrate them into intelligent\napplications. Deploying a model makes it available as a service that you can access by using an\nAPI.\nPipelines for automating your ML workflow.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using OpenShift AI groups, you are part of the user group or admin group (for\nexample, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have the appropriate roles and permissions to create projects.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, select \nData science projects\n.\nThe \nData science projects\n page shows a list of projects that you can access. For each user-\nrequested project in the list, the \nName\n column shows the project display name, the user who\nrequested the project, and the project description.\n2\n. \nClick \nCreate project\n.\n3\n. \nIn the \nCreate project\n dialog, update the \nName\n field to enter a unique display name for your\nproject.\n4\n. \nOptional: If you want to change the default resource name for your project, click \nEdit resource\nname\n.\nThe resource name is what your resource is labeled in OpenShift. Valid characters include\nlowercase letters, numbers, and hyphens (-). The resource name cannot exceed 30 characters,\nand it must start with a letter and end with a letter or number.\nNote:\n You cannot change the resource name after the project is created. You can edit only the\ndisplay name and the description.\n5\n. \nOptional: In the \nDescription\n field, provide a project description.\n6\n. \nClick \nCreate\n.\nVerification\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n10\nA project details page opens. From this page, you can add connections, create workbenches,\nconfigure pipelines, and deploy models.\nCHAPTER 3. CREATING A DATA SCIENCE PROJECT\n11\nCHAPTER 4. CREATING A WORKBENCH AND SELECTING AN\nIDE\nA workbench is an isolated area where you can examine and work with ML models. You can also work\nwith data and run programs, for example to prepare and clean data. While a workbench is not required if,\nfor example, you only want to service an existing model, one is needed for most data science workflow\ntasks, such as writing code to process data or training a model.\nWhen you create a workbench, you specify an image (an IDE, packages, and other dependencies).\nSupported IDEs include JupyterLab, code-server, and RStudio (Technology Preview).\nThe IDEs are based on a server-client architecture. Each IDE provides a server that runs in a container\non the OpenShift cluster, while the user interface (the client) is displayed in your web browser. For\nexample, the Jupyter workbench runs in a container on the Red Hat OpenShift cluster. The client is the\nJupyterLab interface that opens in your web browser on your local computer. All of the commands that\nyou enter in JupyterLab are executed by the workbench. Similarly, other IDEs like code-server or\nRStudio Server provide a server that runs in a container on the OpenShift cluster, while the user\ninterface is displayed in your web browser. This architecture allows you to interact through your local\ncomputer in a browser environment, while all processing occurs on the cluster. The cluster provides the\nbenefits of larger available resources and security because the data being processed never leaves the\ncluster.\nIn a workbench, you can also configure connections (to access external data for training models and to\nsave models so that you can deploy them) and cluster storage (for persisting data). Workbenches within\nthe same project can share models and data through object storage with the data science pipelines and\nmodel servers.\nFor data science projects that require data retention, you can add container storage to the workbench\nyou are creating.\nWithin a project, you can create multiple workbenches. When to create a new workbench depends on\nconsiderations, such as the following:\nThe workbench configuration (for example, CPU, RAM, or IDE). If you want to avoid editing the\nconfiguration of an existing workbench’s configuration to accommodate a new task, you can\ncreate a new workbench instead.\nSeparation of tasks or activities. For example, you might want to use one workbench for your\nLarge Language Models (LLM) experimentation activities, another workbench dedicated to a\ndemo, and another workbench for testing.\n4.1. ABOUT WORKBENCH IMAGES\nA workbench image is optimized with the tools and libraries that you need for model development. You\ncan use the provided workbench images or an OpenShift AI administrator can create custom workbench\nimages adapted to your needs.\nTo provide a consistent, stable platform for your model development, many provided workbench images\ncontain the same version of Python. Most workbench images available on OpenShift AI are pre-built and\nready for you to use immediately after OpenShift AI is installed or upgraded.\nFor information about Red Hat support of workbench images and packages, see \nRed Hat OpenShift AI:\nSupported Configurations\n.\nThe following table lists the workbench images that are installed with Red Hat OpenShift AI by default.\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n12\nIf the preinstalled packages that are provided in these images are not sufficient for your use case, you\nhave the following options:\nInstall additional libraries after launching a default image. This option is good if you want to add\nlibraries on an ad hoc basis as you develop models. However, it can be challenging to manage\nthe dependencies of installed libraries and your changes are not saved when the workbench\nrestarts.\nCreate a custom image that includes the additional libraries or packages. For more information,\nsee \nCreating custom workbench images\n.\nIMPORTANT\nWorkbench images denoted with \n(Technology Preview)\n in this table are not supported\nwith Red Hat production service level agreements (SLAs) and might not be functionally\ncomplete. Red Hat does not recommend using Technology Preview features in\nproduction. These features provide early access to upcoming product features, enabling\ncustomers to test functionality and provide feedback during the development process.\nFor more information about the support scope of Red Hat Technology Preview features,\nsee \nTechnology Preview Features Support Scope\n.\nTable 4.1. Default workbench images\nImage name\nDescription\nCUDA\nIf you are working with compute-intensive data science models that require GPU support,\nuse the Compute Unified Device Architecture (CUDA) workbench image to gain access to\nthe NVIDIA CUDA Toolkit. Using this toolkit, you can optimize your work by using GPU-\naccelerated libraries and optimization tools.\nStandard Data\nScience\nUse the Standard Data Science workbench image for models that do not require\nTensorFlow or PyTorch. This image contains commonly-used libraries to assist you in\ndeveloping your machine learning models.\nTensorFlow\nTensorFlow is an open source platform for machine learning. With TensorFlow, you can\nbuild, train and deploy your machine learning models. TensorFlow contains advanced data\nvisualization features, such as computational graph visualizations. It also allows you to\neasily monitor and track the progress of your models.\nPyTorch\nPyTorch is an open source machine learning library optimized for deep learning. If you are\nworking with computer vision or natural language processing models, use the Pytorch\nworkbench image.\nMinimal Python\nIf you do not require advanced machine learning features, or additional resources for\ncompute-intensive data science work, you can use the Minimal Python image to develop\nyour models.\nTrustyAI\nUse the TrustyAI workbench image to leverage your data science work with model\nexplainability, tracing, and accountability, and runtime monitoring. See the \nTrustyAI\nExplainability repository\n for some example Jupyter notebooks.\nCHAPTER 4. CREATING A WORKBENCH AND SELECTING AN IDE\n13\ncode-server\nWith the code-server workbench image, you can customize your workbench environment\nto meet your needs using a variety of extensions to add new languages, themes,\ndebuggers, and connect to additional services. Enhance the efficiency of your data science\nwork with syntax highlighting, auto-indentation, and bracket matching, as well as an\nautomatic task runner for seamless automation. For more information, see \ncode-server in\nGitHub\n.\nNOTE: Elyra-based pipelines are not available with the code-server workbench image.\nRStudio Server\n(Technology\npreview)\nUse the RStudio Server workbench image to access the RStudio IDE, an integrated\ndevelopment environment for R, a programming language for statistical computing and\ngraphics. For more information, see \nthe RStudio Server site\n.\nTo use the \nRStudio Server\n workbench image, you must first build it by creating a secret\nand triggering the BuildConfig, and then enable it in the OpenShift AI UI by editing the \nrstudio-rhel9\n image stream. For more information, see \nBuilding the RStudio Server\nworkbench images\n.\nIMPORTANT\nDisclaimer:\nRed Hat supports managing workbenches in OpenShift AI. However,\nRed Hat does not provide support for the RStudio software. RStudio\nServer is available through \nhttps://rstudio.org/\n and is subject to RStudio\nlicensing terms. Review the licensing terms before you use this sample\nworkbench.\nImage name\nDescription\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n14\nCUDA -\nRStudio Server\n(Technology\nPreview)\nUse the CUDA - RStudio Server workbench image to access the RStudio IDE and NVIDIA\nCUDA Toolkit. RStudio is an integrated development environment for R, a programming\nlanguage for statistical computing and graphics. With the NVIDIA CUDA toolkit, you can\noptimize your work using GPU-accelerated libraries and optimization tools. For more\ninformation, see \nthe RStudio Server site\n.\nTo use the \nCUDA - RStudio Server\n workbench image, you must first build it by creating a\nsecret and triggering the BuildConfig, and then enable it in the OpenShift AI UI by editing\nthe \ncuda-rstudio-rhel9\n image stream. For more information, see \nBuilding the RStudio\nServer workbench images\n.\nIMPORTANT\nDisclaimer:\nRed Hat supports managing workbenches in OpenShift AI. However,\nRed Hat does not provide support for the RStudio software. RStudio\nServer is available through \nhttps://rstudio.org/\n and is subject to RStudio\nlicensing terms. Review the licensing terms before you use this sample\nworkbench.\nThe \nCUDA - RStudio Server\n workbench image contains NVIDIA CUDA\ntechnology. CUDA licensing information is available at\nhttps://docs.nvidia.com/cuda/\n. Review the licensing terms before you use\nthis sample workbench.\nROCm\nUse the ROCm workbench image to run AI and machine learning workloads on AMD GPUs\nin OpenShift AI. It includes ROCm libraries and tools optimized for high-performance GPU\nacceleration, supporting custom AI workflows and data processing tasks. Use this image\nintegrating additional frameworks or dependencies tailored to your specific AI\ndevelopment needs.\nROCm-\nPyTorch\nUse the ROCm-PyTorch workbench image to optimize PyTorch workloads on AMD GPUs\nin OpenShift AI. It includes ROCm-accelerated PyTorch libraries, enabling efficient deep\nlearning training, inference, and experimentation. This image is designed for data scientists\nworking with PyTorch-based workflows, offering integration with GPU scheduling.\nROCm-\nTensorFlow\nUse the ROCm-TensorFlow workbench image to optimize TensorFlow workloads on AMD\nGPUs in OpenShift AI. It includes ROCm-accelerated TensorFlow libraries to support high-\nperformance deep learning model training and inference. This image simplifies TensorFlow\ndevelopment on AMD GPUs and integrates with OpenShift AI for resource scaling and\nmanagement.\nImage name\nDescription\n4.2. BUILDING THE RSTUDIO SERVER WORKBENCH IMAGES\nIMPORTANT\nCHAPTER 4. CREATING A WORKBENCH AND SELECTING AN IDE\n15\nIMPORTANT\nThe \nRStudio Server\n and \nCUDA - RStudio Server\n workbench images are currently\navailable in Red Hat OpenShift AI as Technology Preview features.\nNOTE\nThe RStudio Server workbench images are currently unavailable for\ndisconnected environments.\nTechnology Preview features are not supported with Red Hat production service level\nagreements (SLAs) and might not be functionally complete. Red Hat does not\nrecommend using them in production. These features provide early access to upcoming\nproduct features, enabling customers to test functionality and provide feedback during\nthe development process.\nFor more information about the support scope of Red Hat Technology Preview features,\nsee \nTechnology Preview Features Support Scope\n.\nRed Hat OpenShift AI includes the following RStudio Server workbench images:\nRStudio Server workbench image\nWith the \nRStudio Server\n workbench image, you can access the RStudio IDE, an integrated\ndevelopment environment for the R programming language. R is used for statistical computing\nand graphics to support data analysis and predictions.\nIMPORTANT\nDisclaimer:\n Red Hat supports managing workbenches in OpenShift AI. However,\nRed Hat does not provide support for the RStudio software. RStudio Server is\navailable through \nrstudio.org\n and is subject to their licensing terms. You should\nreview their licensing terms before you use this sample workbench.\nCUDA - RStudio Server workbench image\nWith the \nCUDA - RStudio Server\n workbench image, you can access the RStudio IDE and\nNVIDIA CUDA Toolkit. The RStudio IDE is an integrated development environment for the R\nprogramming language for statistical computing and graphics. With the NVIDIA CUDA toolkit,\nyou can enhance your work by using GPU-accelerated libraries and optimization tools.\nIMPORTANT\nDisclaimer:\n Red Hat supports managing workbenches in OpenShift AI. However,\nRed Hat does not provide support for the RStudio software. RStudio Server is\navailable through \nrstudio.org\n and is subject to their licensing terms. You should\nreview their licensing terms before you use this sample workbench.\nThe \nCUDA - RStudio Server\n workbench image contains NVIDIA CUDA\ntechnology. CUDA licensing information is available in the \nCUDA Toolkit\ndocumentation\n. You should review their licensing terms before you use this\nsample workbench.\nTo use the \nRStudio Server\n and \nCUDA - RStudio Server\n workbench images, you must first build them\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n16\nTo use the \nRStudio Server\n and \nCUDA - RStudio Server\n workbench images, you must first build them\nby creating a secret and triggering the \nBuildConfig\n, and then enable them in the OpenShift AI UI by\nediting the \nrstudio-rhel9\n and \ncuda-rstudio-rhel9\n image streams.\nPrerequisites\nBefore starting the RStudio Server build process, you have at least 1 CPU and 2Gi memory\navailable for \nrstudio-server-rhel9\n, and 1.5 CPUs and 8Gi memory available for \ncuda-rstudio-\nserver-rhel9\n on your cluster.\nYou are logged in to your OpenShift cluster.\nYou have the \ncluster-admin\n role in OpenShift.\nYou have an active Red Hat Enterprise Linux (RHEL) subscription.\nProcedure\n1\n. \nCreate a secret with Subscription Manager credentials. These are usually your Red Hat\nCustomer Portal username and password.\nNote: The secret must be named \nrhel-subscription-secret\n, and its \nUSERNAME\n and \nPASSWORD\n keys must be in capital letters.\noc create secret generic rhel-subscription-secret --from-literal=USERNAME=<username> --\nfrom-literal=PASSWORD=<password> -n redhat-ods-applications\n2\n. \nStart the build:\na\n. \nTo start the lightweight RStudio Server build:\noc start-build rstudio-server-rhel9 -n redhat-ods-applications --follow\nb\n. \nTo start the CUDA-enabled RStudio Server build, trigger the \ncuda-rstudio-server-rhel9\nBuildConfig:\noc start-build cuda-rstudio-server-rhel9 -n redhat-ods-applications --follow\n3\n. \nConfirm that the build process has completed successfully using the following command.\nSuccessful builds appear as \nComplete\n.\noc get builds -n redhat-ods-applications\n4\n. \nAfter the builds complete successfully, use the following commands to make the workbench\nimages available in the OpenShift AI UI.\na\n. \nTo enable the RStudio Server workbench image:\noc label -n redhat-ods-applications imagestream rstudio-rhel9 opendatahub.io/notebook-\nimage='true'\nb\n. \nTo enable the CUDA - RStudio Server workbench image:\noc label -n redhat-ods-applications imagestream cuda-rstudio-rhel9\n \nopendatahub.io/notebook-image='true'\nCHAPTER 4. CREATING A WORKBENCH AND SELECTING AN IDE\n17\nVerification\nYou can see \nRStudio Server\n and \nCUDA - RStudio Server\n images on the \nApplications\n \n→\nEnabled\n menu in the Red Hat OpenShift AI dashboard.\nYou can see \nR Studio Server\n or \nCUDA - RStudio Server\n in the \nData science projects\n \n→\nWorkbenches\n \n→\n \nCreate workbench\n \n→\n \nNotebook image\n \n→\n \nImage selection\n dropdown list.\n4.3. CREATING A WORKBENCH\nWhen you create a workbench, you specify an image (an IDE, packages, and other dependencies). You\ncan also configure connections, cluster storage, and add container storage.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you use OpenShift AI groups, you are part of the user group or admin group (for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have created a project.\nIf you created a Simple Storage Service (S3) account outside of Red Hat OpenShift AI and you\nwant to create connections to your existing S3 storage buckets, you have the following\ncredential information for the storage buckets:\nEndpoint URL\nAccess key\nSecret key\nRegion\nBucket name\nFor more information, see \nWorking with data in an S3-compatible object store\n.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData science projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project that you want to add the workbench to.\nA project details page opens.\n3\n. \nClick the \nWorkbenches\n tab.\n4\n. \nClick \nCreate workbench\n.\nThe \nCreate workbench\n page opens.\n5\n. \nIn the \nName\n field, enter a unique name for your workbench.\n6\n. \nOptional: If you want to change the default resource name for your workbench, click \nEdit\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n18\n6\n. \nOptional: If you want to change the default resource name for your workbench, click \nEdit\nresource name\n.\nThe resource name is what your resource is labeled in OpenShift. Valid characters include\nlowercase letters, numbers, and hyphens (-). The resource name cannot exceed 30 characters,\nand it must start with a letter and end with a letter or number.\nNote:\n You cannot change the resource name after the workbench is created. You can edit only\nthe display name and the description.\n7\n. \nOptional: In the \nDescription\n field, enter a description for your workbench.\n8\n. \nIn the \nWorkbench image\n section, complete the fields to specify the workbench image to use\nwith your workbench.\nFrom the \nImage selection\n list, select a workbench image that suits your use case. A workbench\nimage includes an IDE and Python packages (reusable code). If project-scoped images exist,\nthe \nImage selection\n list includes subheadings to distinguish between global images and\nproject-scoped images.\nOptionally, click \nView package information\n to view a list of packages that are included in the\nimage that you selected.\nIf the workbench image has multiple versions available, select the workbench image version to\nuse from the \nVersion selection\n list. To use the latest package versions, Red Hat recommends\nthat you use the most recently added image.\nNOTE\nYou can change the workbench image after you create the workbench.\n9\n. \nIn the \nDeployment size\n section, select one of the following options, depending on whether the\nhardware profiles feature is enabled.\nIMPORTANT\nThe hardware profiles feature is currently available in Red Hat OpenShift AI 2.22\nas a Technology Preview feature. Technology Preview features are not supported\nwith Red Hat production service level agreements (SLAs) and might not be\nfunctionally complete. Red Hat does not recommend using them in production.\nThese features provide early access to upcoming product features, enabling\ncustomers to test functionality and provide feedback during the development\nprocess.\nFor more information about the support scope of Red Hat Technology Preview\nfeatures, see \nTechnology Preview Features Support Scope\n.\nIf the hardware profiles feature is not enabled:\na\n. \nFrom the \nContainer size\n list, select the appropriate size for the size of the model that\nyou want to train or tune.\nFor example, to run the example fine-tuning job described in \nFine-tuning a model by\nusing Kubeflow Training\n, select \nMedium\n.\nb\n. \nFrom the \nAccelerator\n list, select a suitable accelerator profile for your workbench.\nIf project-scoped accelerator profiles exist, the \nAccelerator\n list includes subheadings\nCHAPTER 4. CREATING A WORKBENCH AND SELECTING AN IDE\n19\nIf project-scoped accelerator profiles exist, the \nAccelerator\n list includes subheadings\nto distinguish between global accelerator profiles and project-scoped accelerator\nprofiles.\nIf the hardware profiles feature is enabled:\na\n. \nFrom the \nHardware profile\n list, select a suitable hardware profile for your workbench.\nIf project-scoped hardware profiles exist, the \nHardware profile\n list includes\nsubheadings to distinguish between global hardware profiles and project-scoped\nhardware profiles.\nThe hardware profile specifies the number of CPUs and the amount of memory\nallocated to the container, setting the guaranteed minimum (request) and maximum\n(limit) for both.\nb\n. \nIf you want to change the default values, click \nCustomize resource requests and limit\nand enter new minimum (request) and maximum (limit) values.\nIMPORTANT\nBy default, the hardware profiles feature is not enabled: hardware\nprofiles are not shown in the dashboard navigation menu or elsewhere in\nthe user interface. In addition, user interface components associated\nwith the deprecated accelerator profiles functionality are still displayed.\nTo show the \nSettings\n \n→\n \nHardware profiles\n option in the dashboard\nnavigation menu, and the user interface components associated with\nhardware profiles, set the \ndisableHardwareProfiles\n value to \nfalse\n in the\nOdhDashboardConfig\n custom resource (CR) in OpenShift. For more\ninformation about setting dashboard configuration options, see\nCustomizing the dashboard\n.\n10\n. \nOptional: In the \nEnvironment variables\n section, select and specify values for any environment\nvariables.\nSetting environment variables during the workbench configuration helps you save time later\nbecause you do not need to define them in the body of your workbenches, or with the IDE\ncommand line interface.\nIf you are using S3-compatible storage, add these recommended environment variables:\nAWS_ACCESS_KEY_ID\n specifies your Access Key ID for Amazon Web Services.\nAWS_SECRET_ACCESS_KEY\n specifies your Secret access key for the account specified\nin \nAWS_ACCESS_KEY_ID\n.\nOpenShift AI stores the credentials as Kubernetes secrets in a protected namespace if you\nselect \nSecret\n when you add the variable.\n11\n. \nIn the \nCluster storage\n section, configure the storage for your workbench. Select one of the\nfollowing options:\nCreate new persistent storage\n to create storage that is retained after you shut down your\nworkbench. Complete the relevant fields to define the storage:\na\n. \nEnter a \nname\n for the cluster storage.\nb\n. \nEnter a \ndescription\n for the cluster storage.\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n20\nc\n. \nSelect a \nstorage class\n for the cluster storage.\nNOTE\nYou cannot change the storage class after you add the cluster storage to\nthe workbench.\nd\n. \nFor storage classes that support multiple access modes, select an \nAccess mode\n to\ndefine how the volume can be accessed. For more information, see \nAbout persistent\nstorage\n.\nOnly the access modes that have been enabled for the storage class by your cluster and\nOpenShift AI administrators are visible.\ne\n. \nUnder \nPersistent storage size\n, enter a new size in gibibytes or mebibytes.\nUse existing persistent storage\n to reuse existing storage and select the storage from the\nPersistent storage\n list.\n12\n. \nOptional: You can add a connection to your workbench. A connection is a resource that contains\nthe configuration parameters needed to connect to a data source or sink, such as an object\nstorage bucket. You can use storage buckets for storing data, models, and pipeline artifacts.\nYou can also use a connection to specify the location of a model that you want to deploy.\nIn the \nConnections\n section, use an existing connection or create a new connection:\nUse an existing connection as follows:\na\n. \nClick \nAttach existing connections\n.\nb\n. \nFrom the \nConnection\n list, select a connection that you previously defined.\nCreate a new connection as follows:\na\n. \nClick \nCreate connection\n. The \nAdd connection\n dialog appears.\nb\n. \nFrom the \nConnection type\n drop-down list, select the type of connection. The\nConnection details\n section appears.\nc\n. \nIf you selected \nS3 compatible object storage\n in the preceding step, configure the\nconnection details:\ni\n. \nIn the \nConnection name\n field, enter a unique name for the connection.\nii\n. \nOptional: In the \nDescription\n field, enter a description for the connection.\niii\n. \nIn the \nAccess key\n field, enter the access key ID for the S3-compatible object\nstorage provider.\niv\n. \nIn the \nSecret key\n field, enter the secret access key for the S3-compatible object\nstorage account that you specified.\nv\n. \nIn the \nEndpoint\n field, enter the endpoint of your S3-compatible object storage\nbucket.\nvi\n. \nIn the \nRegion\n field, enter the default region of your S3-compatible object storage\naccount.\nvii\n. \nIn the \nBucket\n field, enter the name of your S3-compatible object storage bucket.\nCHAPTER 4. CREATING A WORKBENCH AND SELECTING AN IDE\n21\nviii\n. \nClick \nCreate\n.\nd\n. \nIf you selected \nURI\n in the preceding step, configure the connection details:\ni\n. \nIn the \nConnection name\n field, enter a unique name for the connection.\nii\n. \nOptional: In the \nDescription\n field, enter a description for the connection.\niii\n. \nIn the \nURI\n field, enter the Uniform Resource Identifier (URI).\niv\n. \nClick \nCreate\n.\n13\n. \nClick \nCreate workbench\n.\nVerification\nThe workbench that you created appears on the \nWorkbenches\n tab for the project.\nAny cluster storage that you associated with the workbench during the creation process\nappears on the \nCluster storage\n tab for the project.\nThe \nStatus\n column on the \nWorkbenches\n tab displays a status of \nStarting\n when the workbench\nserver is starting, and \nRunning\n when the workbench has successfully started.\nOptional: Click the open icon ( \n \n) to open the IDE in a new window.\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n22\nCHAPTER 5. NEXT STEPS\nThe following product documentation provides more information on how to develop, test, and deploy\ndata science solutions with OpenShift AI.\nTry the end-to-end tutorial\nOpenShift AI tutorial - Fraud detection example\nStep-by-step guidance to complete the following tasks with an example fraud detection model:\nExplore a pre-trained fraud detection model by using a Jupyter notebook.\nDeploy the model by using OpenShift AI model serving.\nRefine and train the model by using automated pipelines.\nDevelop and train a model in your workbench IDE\nWorking in your data science IDE\nLearn how to access your workbench IDE (JupyterLab, code-server, or RStudio Server).\nFor the JupyterLab IDE, learn about the following tasks:\nCreating and importing Jupyter notebooks\nUsing Git to collaborate on Jupyter notebooks\nViewing and installing Python packages\nTroubleshooting common problems\nAutomate your ML workflow with pipelines\nWorking with data science pipelines\nEnhance your data science projects on OpenShift AI by building portable machine learning (ML)\nworkflows with data science pipelines, by using Docker containers. Use pipelines for continuous\nretraining and updating of a model based on newly received data.\nDeploy and test a model\nServing models\nDeploy your ML models on your OpenShift cluster to test and then integrate them into intelligent\napplications. When you deploy a model, it is available as a service that you can access by using API\ncalls. You can return predictions based on data inputs that you provide through API calls.\nMonitor and manage models\nServing models\nThe Red Hat OpenShift AI service includes model deployment options for hosting the model on Red\nHat OpenShift Dedicated or Red Hat Openshift Service on AWS for integration into an external\napplication.\nAdd accelerators to optimize performance\nWorking with accelerators\nIf you work with large data sets, you can use accelerators, such as NVIDIA GPUs, AMD GPUs, and\nIntel Gaudi AI accelerators, to optimize the performance of your data science models in OpenShift\nAI. With accelerators, you can scale your work, reduce latency, and increase productivity.\nCHAPTER 5. NEXT STEPS\n23\nImplement distributed workloads for higher performance\nWorking with distributed workloads\nImplement distributed workloads to use multiple cluster nodes in parallel for faster, more efficient\ndata processing and model training.\nExplore extensions\nWorking with connected applications\nExtend your core OpenShift AI solution with integrated third-party applications. Several leading\nAI/ML software technology partners, including Starburst, Intel AI Tools, and IBM are also available\nthrough Red Hat Marketplace.\n5.1. ADDITIONAL RESOURCES\nIn addition to product documentation, Red Hat provides a rich set of learning resources for OpenShift\nAI and supported applications.\nOn the \nResources\n page of the OpenShift AI dashboard, you can use the category links to filter the\nresources for various stages of your data science workflow. For example, click the \nModel serving\ncategory to display resources that describe various methods of deploying models. Click \nAll items\n to\nshow the resources for all categories.\nFor the selected category, you can apply additional options to filter the available resources. For\nexample, you can filter by type, such as how-to articles, quick starts, or tutorials; these resources provide\nthe answers to common questions.\nFor information about Red Hat OpenShift AI support requirements and limitations, see \nRed Hat\nOpenShift AI: Supported Configurations\n.\nRed Hat OpenShift AI Self-Managed 2.22 Getting started with Red Hat OpenShift AI Self-Managed\n24\n",
  "metadata": {
    "filename": "Red_Hat_OpenShift_AI_Self-Managed-2.22-Getting_started_with_Red_Hat_OpenShift_AI_Self-Managed-en-US.pdf",
    "format": "text",
    "processed_at": "2025-07-29T00:04:07.880640",
    "pages": 28,
    "method": "PyPDF2_fallback"
  },
  "processed_at": "2025-07-29T00:04:07.880716",
  "success": true
}